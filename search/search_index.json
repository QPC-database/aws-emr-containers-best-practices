{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the EMR Containers Best Practices Guides. The primary goal of this project is to offer a set of best practices and templates to get started with EMR on EKS. We elected to publish this guidance to GitHub so we could iterate quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community. Contributing \u00b6 We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Introduction"},{"location":"#contributing","text":"We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Contributing"},{"location":"debugging/docs/","text":"","title":"Home"},{"location":"debugging/docs/change-log-level/","text":"Change Log level for Spark application on EMR on EKS \u00b6 Spark application developer would want to change log level to different levels depending on their requirement. spark uses apache log4j for logging. Change log level to DEBUG for spark application submitted to EMR-Containers \u00b6 1. Using EMR classification: Log level of spark applications can be changed using the EMR spark-log4j configuration classification. Request cat > Spark - Python - in - s3 - debug - log . json << EOF { \"name\" : \"spark-python-in-s3-debug-log\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/trip-count-repartition-fsx.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.driver.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.executor.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-debug-log.json The above request will print DEBUG logs in the spark driver and executor logs stored in S3 and Cloudwatch logs as configured. Custom log4j properties: Download log4j properties from here . Edit log4j.properties with log level as required. Save the edited log4j.properties in a mounted volume. In this example log4j.properties is placed in a s3 bucket that is mapped to a FSx for Lustre filesystem. Request cat > Spark - Python - in - s3 - debug - log . json << EOF { \"name\" : \"spark-python-in-s3-debug-log\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/trip-count-repartition-fsx.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.driver.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.executor.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-debug-log.json Configurations of interest: Below configuration makes spark driver and executor to pickup the log4j configuration file from /var/data/ folder mounted to the driver and executor containers. For guide to mount FSx for Lustre to driver and executor containers - refer to EMR Containers integration with FSx for Lustre \"spark.driver.extraJavaOptions\":\"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\", \"spark.executor.extraJavaOptions\":\"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\",","title":"Change Log Level"},{"location":"debugging/docs/change-log-level/#change-log-level-for-spark-application-on-emr-on-eks","text":"Spark application developer would want to change log level to different levels depending on their requirement. spark uses apache log4j for logging.","title":"Change Log level for Spark application on EMR on EKS"},{"location":"debugging/docs/change-log-level/#change-log-level-to-debug-for-spark-application-submitted-to-emr-containers","text":"1. Using EMR classification: Log level of spark applications can be changed using the EMR spark-log4j configuration classification. Request cat > Spark - Python - in - s3 - debug - log . json << EOF { \"name\" : \"spark-python-in-s3-debug-log\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/trip-count-repartition-fsx.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.driver.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.executor.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-debug-log.json The above request will print DEBUG logs in the spark driver and executor logs stored in S3 and Cloudwatch logs as configured. Custom log4j properties: Download log4j properties from here . Edit log4j.properties with log level as required. Save the edited log4j.properties in a mounted volume. In this example log4j.properties is placed in a s3 bucket that is mapped to a FSx for Lustre filesystem. Request cat > Spark - Python - in - s3 - debug - log . json << EOF { \"name\" : \"spark-python-in-s3-debug-log\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/trip-count-repartition-fsx.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.driver.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.executor.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-debug-log.json Configurations of interest: Below configuration makes spark driver and executor to pickup the log4j configuration file from /var/data/ folder mounted to the driver and executor containers. For guide to mount FSx for Lustre to driver and executor containers - refer to EMR Containers integration with FSx for Lustre \"spark.driver.extraJavaOptions\":\"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\", \"spark.executor.extraJavaOptions\":\"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\",","title":"Change log level to DEBUG for spark application submitted to EMR-Containers"},{"location":"metastore-integrations/docs/","text":"","title":"Home"},{"location":"metastore-integrations/docs/aws-glue/","text":"EMR Containers integration with AWS Glue \u00b6 Submit spark jobs and configure to use AWS Glue catalog as the hive metastore gluequery.py cat > gluequery . py << EOF from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Hive integration example\" ) \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"CREATE EXTERNAL TABLE `sparkemrnyc`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/trip-data.parquet/'\" ) spark . sql ( \"SELECT count(*) FROM sparkemrnyc\" ) . show () spark . stop () EOF LOCATION 's3://<s3 prefix>/trip-data.parquet/' Configure the above property to point to the location containing the data. Request cat > Spark - Python - in - s3 - awsglue - log . json << EOF { \"name\" : \"spark-python-in-s3-awsglue-log\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/gluequery.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=3 --conf spark.executor.memory=8G --conf spark.driver.memory=6G --conf spark.executor.cores=3\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.hadoop.hive.metastore.client.factory.class\" : \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" , } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-awsglue-log.json Output from driver logs - will have the below log showing the number of rows. +----------+ | count(1)| +----------+ |2716504499| +----------+ In this example spark job connected to the AWS Glue in the same account. Spark Job to connect to AWS Glue catalog in a different account Spark job is running in EKS in Account A and accessing the AWS Glue catalog in Account B ( Refer https://docs.aws.amazon.com/glue/latest/dg/cross-account-access.html) IAM policy attached to the job execution role (\"executionRoleArn\": \"<execution-role-arn>\") in Account A { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"glue:*\" ], \"Resource\": [ \"arn:aws:glue:<region>:<account>:catalog\", \"arn:aws:glue:<region>:<account>:database/default\", \"arn:aws:glue:<region>:<account>:table/default/sparkemrnyc\" ] } ] } IAM policy attached to the AWS Glue catalog in Account B { \"Version\" : \"2012-10-17\", \"Statement\" : [ { \"Effect\" : \"Allow\", \"Principal\" : { \"AWS\" : \"<execution-role-arn>\" }, \"Action\" : \"glue:*\", \"Resource\" : [ \"arn:aws:glue:<region>:<account>:catalog\", \"arn:aws:glue:<region>:<account>:database/default\", \"arn:aws:glue:<region>:<account>:table/default/sparkemrnyc\" ] } ] } Request cat > Spark - Python - in - s3 - awsglue - crossaccount . json << EOF { \"name\" : \"spark-python-in-s3-awsglue-crossaccount\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/gluequery.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 \" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.hadoop.hive.metastore.client.factory.class\" : \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" , ** \"spark.hadoop.hive.metastore.glue.catalogid\" : \"<account B>\" ** , } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-awsglue-crossaccount.json Configuration of interest - Specify the account id where the AWS Glue catalog is defined(https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-glue.html) \"spark.hadoop.hive.metastore.glue.catalogid\":\"<account B>\", Output from driver logs - will have the below log showing the number of rows. +----------+ | count(1)| +----------+ |2716504499| +----------+","title":"AWS Glue"},{"location":"metastore-integrations/docs/aws-glue/#emr-containers-integration-with-aws-glue","text":"Submit spark jobs and configure to use AWS Glue catalog as the hive metastore gluequery.py cat > gluequery . py << EOF from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Hive integration example\" ) \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"CREATE EXTERNAL TABLE `sparkemrnyc`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/trip-data.parquet/'\" ) spark . sql ( \"SELECT count(*) FROM sparkemrnyc\" ) . show () spark . stop () EOF LOCATION 's3://<s3 prefix>/trip-data.parquet/' Configure the above property to point to the location containing the data. Request cat > Spark - Python - in - s3 - awsglue - log . json << EOF { \"name\" : \"spark-python-in-s3-awsglue-log\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/gluequery.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=3 --conf spark.executor.memory=8G --conf spark.driver.memory=6G --conf spark.executor.cores=3\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.hadoop.hive.metastore.client.factory.class\" : \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" , } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-awsglue-log.json Output from driver logs - will have the below log showing the number of rows. +----------+ | count(1)| +----------+ |2716504499| +----------+ In this example spark job connected to the AWS Glue in the same account. Spark Job to connect to AWS Glue catalog in a different account Spark job is running in EKS in Account A and accessing the AWS Glue catalog in Account B ( Refer https://docs.aws.amazon.com/glue/latest/dg/cross-account-access.html) IAM policy attached to the job execution role (\"executionRoleArn\": \"<execution-role-arn>\") in Account A { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"glue:*\" ], \"Resource\": [ \"arn:aws:glue:<region>:<account>:catalog\", \"arn:aws:glue:<region>:<account>:database/default\", \"arn:aws:glue:<region>:<account>:table/default/sparkemrnyc\" ] } ] } IAM policy attached to the AWS Glue catalog in Account B { \"Version\" : \"2012-10-17\", \"Statement\" : [ { \"Effect\" : \"Allow\", \"Principal\" : { \"AWS\" : \"<execution-role-arn>\" }, \"Action\" : \"glue:*\", \"Resource\" : [ \"arn:aws:glue:<region>:<account>:catalog\", \"arn:aws:glue:<region>:<account>:database/default\", \"arn:aws:glue:<region>:<account>:table/default/sparkemrnyc\" ] } ] } Request cat > Spark - Python - in - s3 - awsglue - crossaccount . json << EOF { \"name\" : \"spark-python-in-s3-awsglue-crossaccount\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/gluequery.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 \" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.hadoop.hive.metastore.client.factory.class\" : \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" , ** \"spark.hadoop.hive.metastore.glue.catalogid\" : \"<account B>\" ** , } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-awsglue-crossaccount.json Configuration of interest - Specify the account id where the AWS Glue catalog is defined(https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-glue.html) \"spark.hadoop.hive.metastore.glue.catalogid\":\"<account B>\", Output from driver logs - will have the below log showing the number of rows. +----------+ | count(1)| +----------+ |2716504499| +----------+","title":"EMR Containers integration with AWS Glue"},{"location":"metastore-integrations/docs/hive-metastore/","text":"EMR Containers integration with Hive Metastore \u00b6 Submit spark jobs and configure to use hive metastore thrift server running on EMR cluster Configure Spark to connect to Hive metastore Database through JDBC \u00b6 Use RDS Aurora MySql as Hive Metastore and connect from Spark using JDBC string. The RDS and EKS cluster shouldbe in same VPC or else Spark job will not be able to connect to RDS. Script to submit job: aws emr-containers start-job-run --virtual-cluster-id <virtual-cluster-id> --name sparkjob --execution-role-arn <execution-role-arn> --release-label emr-6.2.0-latest --job-driver '{\"sparkSubmitJobDriver\": {\"entryPoint\": \"s3://<s3 prefix>/hivejdbc.py\", \"sparkSubmitParameters\":\" --jars s3://<s3 prefix>/mariadb-connector-java.jar --conf spark.executor.instances=3 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1 --conf spark.hadoop.javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver --conf spark.hadoop.javax.jdo.option.ConnectionUserName=<connection-user-name> --conf spark.hadoop.javax.jdo.option.ConnectionPassword=<connection-password> --conf spark.hadoop.javax.jdo.option.ConnectionURL=<JDBC-Connection-string>\"}}' --configuration-overrides '{\"applicationConfiguration\": [{\"classification\": \"spark-defaults\",\"properties\": {\"spark.executor.instances\": \"2\",\"spark.executor.memory\": \"2G\"}}],\"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"`/emr-containers/jobs`\", \"logStreamNamePrefix\": \"demo\"}, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs/\" }}}' In this example we are connecting to mysql db, so mariadb-connector-java.jar needs to be passed with --jars option. If you are using postgres, oracle or any other database, appropriate connector jar needs to be included.Pass following for sparkSubmitJobDriver configuration: *--**jars s3**:**//<s3 prefix>/mariadb-connector-java.jar* *--**conf spark**.**hadoop**.**javax**.**jdo**.**option**.**ConnectionDriverName**=**org**.**mariadb**.**jdbc**.**Driver* ** *-**-**conf spark**.**hadoop**.**javax**.**jdo**.**option**.**ConnectionUserName**=<**connection**-**user**-**name**>* ** *--**conf spark**.**hadoop**.**javax**.**jdo**.**option**.**ConnectionPassword**=<**connection**-**password**>* *--**conf spark**.**hadoop**.**javax**.**jdo**.**option**.**ConnectionURL**=<JDBC-Connection-string>* hivejdbc.py from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"SHOW DATABASES\" ) . show () spark . sql ( \"CREATE EXTERNAL TABLE `ehmsdb`.`sparkemrnyc5`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/nyctaxi_parquet/'\" ) spark . sql ( \"SELECT count(*) FROM ehmsdb.sparkemrnyc5 \" ) . show () spark . stop () The above job lists databases from remote RDS Hive Metastore, creates a new table and query it as well. Configure Spark to connect to Hive metastore thrift service through thrift:// protocol \u00b6 Query from tables through an external Hive metastore thrift server. Thrift server is on EMR\u2019s master node and RDS Aurora is used as database for Hive Metastore. Script to submit job: aws emr-containers start-job-run --virtual-cluster-id <virtual-cluster-id> --name sparkjob --execution-role-arn <execution-role-arn> --release-label emr-6.2.0-latest --job-driver '{\"sparkSubmitJobDriver\": {\"entryPoint\": \"s3://<s3 prefix>/thriftscript.py\", \"sparkSubmitParameters\":\" --conf spark.executor.instances=3 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1 \"}}' --configuration-overrides '{\"applicationConfiguration\": [{\"classification\": \"spark-defaults\",\"properties\": {\"spark.executor.instances\": \"2\",\"spark.executor.memory\": \"2G\"}}],\"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"`/emr-containers/jobs`\", \"logStreamNamePrefix\": \"demo\"}, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs/\" }}}' thriftscript.py: hive.metastore.uris config needs to be set to read from external Hive metastore. from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . config ( \"hive.metastore.uris\" , \"<hive metastore thrift uri>\" ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"SHOW DATABASES\" ) . show () spark . sql ( \"CREATE EXTERNAL TABLE ehmsdb.`sparkemrnyc2`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/nyctaxi_parquet/'\" ) spark . sql ( \"SELECT * FROM ehmsdb.sparkemrnyc2\" ) . show () spark . stop () The above job lists databases from remote Hive Metastore, creates a new table and query it as well.","title":"Hive Metastore"},{"location":"metastore-integrations/docs/hive-metastore/#emr-containers-integration-with-hive-metastore","text":"Submit spark jobs and configure to use hive metastore thrift server running on EMR cluster","title":"EMR Containers integration with Hive Metastore"},{"location":"metastore-integrations/docs/hive-metastore/#configure-spark-to-connect-to-hive-metastore-database-through-jdbc","text":"Use RDS Aurora MySql as Hive Metastore and connect from Spark using JDBC string. The RDS and EKS cluster shouldbe in same VPC or else Spark job will not be able to connect to RDS. Script to submit job: aws emr-containers start-job-run --virtual-cluster-id <virtual-cluster-id> --name sparkjob --execution-role-arn <execution-role-arn> --release-label emr-6.2.0-latest --job-driver '{\"sparkSubmitJobDriver\": {\"entryPoint\": \"s3://<s3 prefix>/hivejdbc.py\", \"sparkSubmitParameters\":\" --jars s3://<s3 prefix>/mariadb-connector-java.jar --conf spark.executor.instances=3 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1 --conf spark.hadoop.javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver --conf spark.hadoop.javax.jdo.option.ConnectionUserName=<connection-user-name> --conf spark.hadoop.javax.jdo.option.ConnectionPassword=<connection-password> --conf spark.hadoop.javax.jdo.option.ConnectionURL=<JDBC-Connection-string>\"}}' --configuration-overrides '{\"applicationConfiguration\": [{\"classification\": \"spark-defaults\",\"properties\": {\"spark.executor.instances\": \"2\",\"spark.executor.memory\": \"2G\"}}],\"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"`/emr-containers/jobs`\", \"logStreamNamePrefix\": \"demo\"}, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs/\" }}}' In this example we are connecting to mysql db, so mariadb-connector-java.jar needs to be passed with --jars option. If you are using postgres, oracle or any other database, appropriate connector jar needs to be included.Pass following for sparkSubmitJobDriver configuration: *--**jars s3**:**//<s3 prefix>/mariadb-connector-java.jar* *--**conf spark**.**hadoop**.**javax**.**jdo**.**option**.**ConnectionDriverName**=**org**.**mariadb**.**jdbc**.**Driver* ** *-**-**conf spark**.**hadoop**.**javax**.**jdo**.**option**.**ConnectionUserName**=<**connection**-**user**-**name**>* ** *--**conf spark**.**hadoop**.**javax**.**jdo**.**option**.**ConnectionPassword**=<**connection**-**password**>* *--**conf spark**.**hadoop**.**javax**.**jdo**.**option**.**ConnectionURL**=<JDBC-Connection-string>* hivejdbc.py from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"SHOW DATABASES\" ) . show () spark . sql ( \"CREATE EXTERNAL TABLE `ehmsdb`.`sparkemrnyc5`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/nyctaxi_parquet/'\" ) spark . sql ( \"SELECT count(*) FROM ehmsdb.sparkemrnyc5 \" ) . show () spark . stop () The above job lists databases from remote RDS Hive Metastore, creates a new table and query it as well.","title":"Configure Spark to connect to Hive metastore Database through JDBC"},{"location":"metastore-integrations/docs/hive-metastore/#configure-spark-to-connect-to-hive-metastore-thrift-service-through-thrift-protocol","text":"Query from tables through an external Hive metastore thrift server. Thrift server is on EMR\u2019s master node and RDS Aurora is used as database for Hive Metastore. Script to submit job: aws emr-containers start-job-run --virtual-cluster-id <virtual-cluster-id> --name sparkjob --execution-role-arn <execution-role-arn> --release-label emr-6.2.0-latest --job-driver '{\"sparkSubmitJobDriver\": {\"entryPoint\": \"s3://<s3 prefix>/thriftscript.py\", \"sparkSubmitParameters\":\" --conf spark.executor.instances=3 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1 \"}}' --configuration-overrides '{\"applicationConfiguration\": [{\"classification\": \"spark-defaults\",\"properties\": {\"spark.executor.instances\": \"2\",\"spark.executor.memory\": \"2G\"}}],\"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"`/emr-containers/jobs`\", \"logStreamNamePrefix\": \"demo\"}, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs/\" }}}' thriftscript.py: hive.metastore.uris config needs to be set to read from external Hive metastore. from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . config ( \"hive.metastore.uris\" , \"<hive metastore thrift uri>\" ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"SHOW DATABASES\" ) . show () spark . sql ( \"CREATE EXTERNAL TABLE ehmsdb.`sparkemrnyc2`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/nyctaxi_parquet/'\" ) spark . sql ( \"SELECT * FROM ehmsdb.sparkemrnyc2\" ) . show () spark . stop () The above job lists databases from remote Hive Metastore, creates a new table and query it as well.","title":"Configure Spark to connect to Hive metastore thrift service through thrift:// protocol"},{"location":"node-placement/docs/","text":"","title":"Home"},{"location":"node-placement/docs/eks-node-placement/","text":"Run spark Application in EKS worker nodes in a single AZ \u00b6 EKS cluster can span across multiple AZ in a VPC. Spark application whose driver and executor pods are distributed across multiple AZ incur data transfer cost. Refer Configuration - https://spark.apache.org/docs/latest/running-on-kubernetes.html#pod-spec Kubernetes Node selector reference - https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ Configuration of interest - --conf spark.kubernetes.node.selector.zone='<availability zone>' zone is a built in label that EKS assigns to every EKS worker Node. The above config will make sure to schedule the driver and executor pod on those EKS worker nodes with label - topology.kubernetes.io/zone : . However user defined labels can also be created and used as node selector - Refer https://eksctl.io/usage/eks-managed-nodes/#managing-labels Other common use cases are using node labels to force the job to run on on demand/spot, machine type, etc. Sample Request cat >spark-python-in-s3-nodeselector.json << EOF { \"name\": \"spark-python-in-s3-nodeselector\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='<availability zone>' --conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-nodeselector.json Observed Behavior: When the job gets started the driver pod and executor pods are scheduled only on those EKS worker nodes with the label topology.kubernetes.io/zone : . This ensures the spark job is run within a single AZ. If there are not enough resource within the AZ the pods will be in pending state until the Autoscaler(if configured) to kick in or more resources to be available. Multiple key value pairs for spark.kubernetes.node.selector.[labelKey] can be passed to add more labels for selecting the EKS worker node. If you want to schedule on EKS worker nodes in and instance-type as m5.4xlarge - it is done as below { \"name\": \"spark-python-in-s3-nodeselector\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.kubernetes.pyspark.pythonVersion=3 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 --conf spark.sql.shuffle.partitions=1000\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"false\", \"spark.kubernetes.node.selector.topology.kubernetes.io/zone\":\"<availability zone>\", \"spark.kubernetes.node.selector.node.kubernetes.io/instance-type\":\"m5.4xlarge\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } } } Configuration of interest - http://spark.apache.org/docs/latest/running-on-kubernetes.html spark.kubernetes.node.selector.[labelKey] - Adds to the node selector of the driver pod and executor pods, with key labelKey and the value as the configuration's value. For example, setting spark.kubernetes.node.selector.identifier to myIdentifier will result in the driver pod and executors having a node selector with key identifier and value myIdentifier. Multiple node selector keys can be added by setting multiple configurations with this prefix.","title":"EKS Node placement"},{"location":"node-placement/docs/eks-node-placement/#run-spark-application-in-eks-worker-nodes-in-a-single-az","text":"EKS cluster can span across multiple AZ in a VPC. Spark application whose driver and executor pods are distributed across multiple AZ incur data transfer cost. Refer Configuration - https://spark.apache.org/docs/latest/running-on-kubernetes.html#pod-spec Kubernetes Node selector reference - https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ Configuration of interest - --conf spark.kubernetes.node.selector.zone='<availability zone>' zone is a built in label that EKS assigns to every EKS worker Node. The above config will make sure to schedule the driver and executor pod on those EKS worker nodes with label - topology.kubernetes.io/zone : . However user defined labels can also be created and used as node selector - Refer https://eksctl.io/usage/eks-managed-nodes/#managing-labels Other common use cases are using node labels to force the job to run on on demand/spot, machine type, etc. Sample Request cat >spark-python-in-s3-nodeselector.json << EOF { \"name\": \"spark-python-in-s3-nodeselector\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='<availability zone>' --conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-nodeselector.json Observed Behavior: When the job gets started the driver pod and executor pods are scheduled only on those EKS worker nodes with the label topology.kubernetes.io/zone : . This ensures the spark job is run within a single AZ. If there are not enough resource within the AZ the pods will be in pending state until the Autoscaler(if configured) to kick in or more resources to be available. Multiple key value pairs for spark.kubernetes.node.selector.[labelKey] can be passed to add more labels for selecting the EKS worker node. If you want to schedule on EKS worker nodes in and instance-type as m5.4xlarge - it is done as below { \"name\": \"spark-python-in-s3-nodeselector\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.kubernetes.pyspark.pythonVersion=3 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 --conf spark.sql.shuffle.partitions=1000\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"false\", \"spark.kubernetes.node.selector.topology.kubernetes.io/zone\":\"<availability zone>\", \"spark.kubernetes.node.selector.node.kubernetes.io/instance-type\":\"m5.4xlarge\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } } } Configuration of interest - http://spark.apache.org/docs/latest/running-on-kubernetes.html spark.kubernetes.node.selector.[labelKey] - Adds to the node selector of the driver pod and executor pods, with key labelKey and the value as the configuration's value. For example, setting spark.kubernetes.node.selector.identifier to myIdentifier will result in the driver pod and executors having a node selector with key identifier and value myIdentifier. Multiple node selector keys can be added by setting multiple configurations with this prefix.","title":"Run spark Application in EKS worker nodes in a single AZ"},{"location":"node-placement/docs/fargate-node-placement/","text":"","title":"Fargate node placement"},{"location":"performance/docs/","text":"","title":"Home"},{"location":"performance/docs/dra/","text":"Dynamic Resource Allocation \u00b6 DRA is available in spark 3 (EMR 6.x) without the need for an external shuffle service. EMR containers doesn't support external shuffle service for GA, but DRA can be achieved by enabling shuffle tracking . Spark DRA without external shuffle service: With DRA - spark driver spawns the initial number of executors and then scales the number until the maximum number as needed to process the pending tasks. Once there are no pending tasks and executor idle time exceeds the idle timeout( spark.dynamicAllocation.executorIdleTimeout) and doesn't have any cached or shuffle data, then the idle executor is terminated. If the executor idle threshold is reached and it has cached data, then it has to exceed the executor with cache data idle timeout( spark.dynamicAllocation.cachedExecutorIdleTimeout) and if the executor doesn't have shuffle data, then the idle executor is terminated. If the executor idle threshold is reached and it has shuffle data, then without external shuffle service the executor will never be terminated. This behavior is enforced by \"spark.dynamicAllocation.shuffleTracking.enabled\":\"true\" and \"spark.dynamicAllocation.enabled\":\"true\" If \"spark.dynamicAllocation.shuffleTracking.enabled\":\"false\"and \"spark.dynamicAllocation.enabled\":\"true\" then spark will error out if external shuffle service is not running. Sample Request cat >spark-python-in-s3-dra.json << EOF { \"name\": \"spark-python-in-s3-dra\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"true\", \"spark.dynamicAllocation.shuffleTracking.enabled\":\"true\", \"spark.dynamicAllocation.minExecutors\":\"5\", \"spark.dynamicAllocation.maxExecutors\":\"100\", \"spark.dynamicAllocation.initialExecutors\":\"10\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-dra.json Observed Behavior: When the job gets started the driver pod gets created and then 10 executors are created to start with (\"spark.dynamicAllocation.initialExecutors\":\"10\") and then the number of executors can scale up to a maximum of 100 (\"spark.dynamicAllocation.maxExecutors\":\"100\"). Configurations to note - Please note that this feature is marked as Experimental as of Spark 3.0.0 spark.dynamicAllocation.shuffleTracking.enabled - Experimental . Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service. This option will try to keep alive executors that are storing shuffle data for active jobs. spark.dynamicAllocation.shuffleTracking.timeout - When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle data. The default value means that Spark will rely on the shuffles being garbage collected to be able to release executors. If for some reason garbage collection is not cleaning up shuffles quickly enough, this option can be used to control when to time out executors even when they are storing shuffle data.","title":"Dynamic Resource Allocation"},{"location":"performance/docs/dra/#dynamic-resource-allocation","text":"DRA is available in spark 3 (EMR 6.x) without the need for an external shuffle service. EMR containers doesn't support external shuffle service for GA, but DRA can be achieved by enabling shuffle tracking . Spark DRA without external shuffle service: With DRA - spark driver spawns the initial number of executors and then scales the number until the maximum number as needed to process the pending tasks. Once there are no pending tasks and executor idle time exceeds the idle timeout( spark.dynamicAllocation.executorIdleTimeout) and doesn't have any cached or shuffle data, then the idle executor is terminated. If the executor idle threshold is reached and it has cached data, then it has to exceed the executor with cache data idle timeout( spark.dynamicAllocation.cachedExecutorIdleTimeout) and if the executor doesn't have shuffle data, then the idle executor is terminated. If the executor idle threshold is reached and it has shuffle data, then without external shuffle service the executor will never be terminated. This behavior is enforced by \"spark.dynamicAllocation.shuffleTracking.enabled\":\"true\" and \"spark.dynamicAllocation.enabled\":\"true\" If \"spark.dynamicAllocation.shuffleTracking.enabled\":\"false\"and \"spark.dynamicAllocation.enabled\":\"true\" then spark will error out if external shuffle service is not running. Sample Request cat >spark-python-in-s3-dra.json << EOF { \"name\": \"spark-python-in-s3-dra\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"true\", \"spark.dynamicAllocation.shuffleTracking.enabled\":\"true\", \"spark.dynamicAllocation.minExecutors\":\"5\", \"spark.dynamicAllocation.maxExecutors\":\"100\", \"spark.dynamicAllocation.initialExecutors\":\"10\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-dra.json Observed Behavior: When the job gets started the driver pod gets created and then 10 executors are created to start with (\"spark.dynamicAllocation.initialExecutors\":\"10\") and then the number of executors can scale up to a maximum of 100 (\"spark.dynamicAllocation.maxExecutors\":\"100\"). Configurations to note - Please note that this feature is marked as Experimental as of Spark 3.0.0 spark.dynamicAllocation.shuffleTracking.enabled - Experimental . Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service. This option will try to keep alive executors that are storing shuffle data for active jobs. spark.dynamicAllocation.shuffleTracking.timeout - When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle data. The default value means that Spark will rely on the shuffles being garbage collected to be able to release executors. If for some reason garbage collection is not cleaning up shuffles quickly enough, this option can be used to control when to time out executors even when they are storing shuffle data.","title":"Dynamic Resource Allocation"},{"location":"security/docs/","text":"","title":"Home"},{"location":"security/docs/spark/data-encryption/","text":"EMR Containers Spark - In transit and At Rest data encryption \u00b6 There are many scenarios in which data encryption should be considered. They are: Encryption in Transit - communication between the driver and executor containers Encryption in Transit - communication from driver and executor containers to S3 through emrfs Encryption at Rest - Amazon S3 server-side encryption SSE-S3 SSE-KMS Encryption at Rest - Amazon S3 client-side encryption Encryption at Rest - data persisted to local disk on EKS worker nodes We will be focusing on #4 and #5. 4. Encryption at Rest - Amazon S3 Client-Side Encryption \u00b6 To utilize S3 Client side encryption, you will need to create a KMS Key to be used to encrypt and decrypt data. If you do not have an KMS key, please follow this guide - AWS KMS create keys . Also please note the job execution role needs access to this key, please see Add to Key policy for instructions on how to add these permissions. Example Script 1: cat > trip - count - encrypt - write . py << EOF import sys from pyspark.sql import SparkSession if __name__ == \"__main__\" : spark = SparkSession \\ . builder \\ . appName ( \"trip-count-join-fsx\" ) \\ . getOrCreate () df = spark . read . parquet ( 's3://<s3 prefix>/trip-data.parquet' ) print ( \"Total trips: \" + str ( df . count ())) df . write . parquet ( 's3://<s3 prefix>/write-encrypt-trip-data.parquet' ) print ( \"Encrypt - KMS- CSE writew to s3 compeleted\" ) spark . stop () EOF CSE configuration for EMR on KMS: cat > spark - python - in - s3 - encrypt - cse - kms - write . json << EOF { \"name\" : \"spark-python-in-s3-encrypt-cse-kms-write\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>trip-count-encrypt-write.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=10 --conf spark.driver.cores=2 --conf spark.executor.memory=20G --conf spark.driver.memory=20G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } , { \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.cse.enabled\" : \"true\" , \"fs.s3.cse.encryptionMaterialsProvider\" : \"com.amazon.ws.emr.hadoop.fs.cse.KMSEncryptionMaterialsProvider\" , \"fs.s3.cse.kms.keyId\" : \"<KMS Key Id>\" } } ], \"monitoringConfiguration\" : { \"persistentAppUI\" : \"ENABLED\" , \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-encrypt-cse-kms-write.json In the above request, emrfs encrypts the parquet file with the specified KMS key and the encrypted object is persisted to the specified s3 location. To verify the encryption - use the same KMS key to decrypt - the KMS key used is a symmetric key ( the same key can be used to both encrypt and decrypt) cat > trip - count - encrypt - read . py << EOF import sys from pyspark.sql import SparkSession if __name__ == \"__main__\" : spark = SparkSession \\ . builder \\ . appName ( \"trip-count-join-fsx\" ) \\ . getOrCreate () df = spark . read . parquet ( 's3://<s3 prefix>/trip-data.parquet' ) print ( \"Total trips: \" + str ( df . count ())) df_encrypt = spark . read . parquet ( 's3://<s3 prefix>/write-encrypt-trip-data.parquet' ) print ( \"Encrypt data - Total trips: \" + str ( df_encrypt . count ())) spark . stop () EOF Request cat > spark - python - in - s3 - encrypt - cse - kms - read . json << EOF { \"name\" : \"spark-python-in-s3-encrypt-cse-kms-read\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>trip-count-encrypt-write.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=10 --conf spark.driver.cores=2 --conf spark.executor.memory=20G --conf spark.driver.memory=20G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } , { \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.cse.enabled\" : \"true\" , \"fs.s3.cse.encryptionMaterialsProvider\" : \"com.amazon.ws.emr.hadoop.fs.cse.KMSEncryptionMaterialsProvider\" , \"fs.s3.cse.kms.keyId\" : \"<KMS Key Id>\" } } ], \"monitoringConfiguration\" : { \"persistentAppUI\" : \"ENABLED\" , \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-encrypt-cse-kms-read.json Validate encryption: Try to read the encrypted data without specifying \"fs.s3.cse.enabled\":\"true\" - will get an error message in the driver and executor logs because the content is encrypted and cannot be read without decryption. 5. Encryption at Rest - data persisted to local disk on EKS worker nodes \u00b6 https://spark.apache.org/docs/latest/security.html#local-storage-encryption Spark supports encrypting temporary data written to local disks. This covers shuffle files, shuffle spills and data blocks stored on disk (for both caching and broadcast variables). It does not cover encrypting output data generated by applications with APIs such as saveAsHadoopFile or saveAsTable . It also may not cover temporary files created explicitly by the user.","title":"Data Encryption"},{"location":"security/docs/spark/data-encryption/#emr-containers-spark-in-transit-and-at-rest-data-encryption","text":"There are many scenarios in which data encryption should be considered. They are: Encryption in Transit - communication between the driver and executor containers Encryption in Transit - communication from driver and executor containers to S3 through emrfs Encryption at Rest - Amazon S3 server-side encryption SSE-S3 SSE-KMS Encryption at Rest - Amazon S3 client-side encryption Encryption at Rest - data persisted to local disk on EKS worker nodes We will be focusing on #4 and #5.","title":"EMR Containers Spark - In transit and At Rest data encryption"},{"location":"security/docs/spark/data-encryption/#4-encryption-at-rest-amazon-s3-client-side-encryption","text":"To utilize S3 Client side encryption, you will need to create a KMS Key to be used to encrypt and decrypt data. If you do not have an KMS key, please follow this guide - AWS KMS create keys . Also please note the job execution role needs access to this key, please see Add to Key policy for instructions on how to add these permissions. Example Script 1: cat > trip - count - encrypt - write . py << EOF import sys from pyspark.sql import SparkSession if __name__ == \"__main__\" : spark = SparkSession \\ . builder \\ . appName ( \"trip-count-join-fsx\" ) \\ . getOrCreate () df = spark . read . parquet ( 's3://<s3 prefix>/trip-data.parquet' ) print ( \"Total trips: \" + str ( df . count ())) df . write . parquet ( 's3://<s3 prefix>/write-encrypt-trip-data.parquet' ) print ( \"Encrypt - KMS- CSE writew to s3 compeleted\" ) spark . stop () EOF CSE configuration for EMR on KMS: cat > spark - python - in - s3 - encrypt - cse - kms - write . json << EOF { \"name\" : \"spark-python-in-s3-encrypt-cse-kms-write\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>trip-count-encrypt-write.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=10 --conf spark.driver.cores=2 --conf spark.executor.memory=20G --conf spark.driver.memory=20G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } , { \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.cse.enabled\" : \"true\" , \"fs.s3.cse.encryptionMaterialsProvider\" : \"com.amazon.ws.emr.hadoop.fs.cse.KMSEncryptionMaterialsProvider\" , \"fs.s3.cse.kms.keyId\" : \"<KMS Key Id>\" } } ], \"monitoringConfiguration\" : { \"persistentAppUI\" : \"ENABLED\" , \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-encrypt-cse-kms-write.json In the above request, emrfs encrypts the parquet file with the specified KMS key and the encrypted object is persisted to the specified s3 location. To verify the encryption - use the same KMS key to decrypt - the KMS key used is a symmetric key ( the same key can be used to both encrypt and decrypt) cat > trip - count - encrypt - read . py << EOF import sys from pyspark.sql import SparkSession if __name__ == \"__main__\" : spark = SparkSession \\ . builder \\ . appName ( \"trip-count-join-fsx\" ) \\ . getOrCreate () df = spark . read . parquet ( 's3://<s3 prefix>/trip-data.parquet' ) print ( \"Total trips: \" + str ( df . count ())) df_encrypt = spark . read . parquet ( 's3://<s3 prefix>/write-encrypt-trip-data.parquet' ) print ( \"Encrypt data - Total trips: \" + str ( df_encrypt . count ())) spark . stop () EOF Request cat > spark - python - in - s3 - encrypt - cse - kms - read . json << EOF { \"name\" : \"spark-python-in-s3-encrypt-cse-kms-read\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>trip-count-encrypt-write.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=10 --conf spark.driver.cores=2 --conf spark.executor.memory=20G --conf spark.driver.memory=20G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } , { \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.cse.enabled\" : \"true\" , \"fs.s3.cse.encryptionMaterialsProvider\" : \"com.amazon.ws.emr.hadoop.fs.cse.KMSEncryptionMaterialsProvider\" , \"fs.s3.cse.kms.keyId\" : \"<KMS Key Id>\" } } ], \"monitoringConfiguration\" : { \"persistentAppUI\" : \"ENABLED\" , \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-encrypt-cse-kms-read.json Validate encryption: Try to read the encrypted data without specifying \"fs.s3.cse.enabled\":\"true\" - will get an error message in the driver and executor logs because the content is encrypted and cannot be read without decryption.","title":"4. Encryption at Rest - Amazon S3 Client-Side Encryption"},{"location":"security/docs/spark/data-encryption/#5-encryption-at-rest-data-persisted-to-local-disk-on-eks-worker-nodes","text":"https://spark.apache.org/docs/latest/security.html#local-storage-encryption Spark supports encrypting temporary data written to local disks. This covers shuffle files, shuffle spills and data blocks stored on disk (for both caching and broadcast variables). It does not cover encrypting output data generated by applications with APIs such as saveAsHadoopFile or saveAsTable . It also may not cover temporary files created explicitly by the user.","title":"5. Encryption at Rest - data persisted to local disk on EKS worker nodes"},{"location":"security/docs/spark/network-security/","text":"","title":"Network security"},{"location":"storage/docs/","text":"","title":"Home"},{"location":"storage/docs/spark/ebs/","text":"Mount EBS Volume to spark driver and executor pods \u00b6 EBS Volumes can be mounted on spark driver and executor pods through static and dynamic provisioning - Difference between static and dynamic provisioning of Persistent Volumes: (Refer - https://kubernetes.io/docs/concepts/storage/persistent-volumes/#provisioning) EKS support for EBS CSI driver - https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html Documentation for EBS CSI driver - https://github.com/kubernetes-sigs/aws-ebs-csi-driver Static Provisioning - \u00b6 EKS Admin Tasks: \u00b6 Create EBS volumes aws ec2 --region <region> create-volume --availability-zone <availability zone> --size 50 { \"AvailabilityZone\": \"<availability zone>\", \"MultiAttachEnabled\": false, \"Tags\": [], \"Encrypted\": false, \"VolumeType\": \"gp2\", \"VolumeId\": \"<vol -id>\", \"State\": \"creating\", \"Iops\": 150, \"SnapshotId\": \"\", \"CreateTime\": \"2020-11-03T18:36:21.000Z\", \"Size\": 50 } Create Persistent Volume(PV) that has the EBS volume created above hardcoded cat > ebs - static - pv . yaml << EOF apiVersion : v1 kind : PersistentVolume metadata : name : ebs - static - pv spec : capacity : storage : 5 Gi accessModes : - ReadWriteOnce storageClassName : gp2 awsElasticBlockStore : fsType : ext4 volumeID : < vol - id > EOF kubectl apply - f ebs - static - pv . yaml - n < namespace > Create Persistent Volume Claim(PVC) for the Persistent Volume created above cat > ebs - static - pvc . yaml << EOF kind : PersistentVolumeClaim apiVersion : v1 metadata : name : ebs - static - pvc spec : accessModes : - ReadWriteOnce resources : requests : storage : 5 Gi volumeName : ebs - static - pv EOF kubectl apply - f ebs - static - pvc . yaml - n < namespace > PVC - ebs-static-pvc can be used by spark developer to mount to the spark pod NOTE: Pods running in EKS worker nodes can attach to only EBS volume provisioned in the same AZ as the EKS worker Node. Use node selector to schedule pods on a EKS worker node in a specified AZ Spark Developer Tasks: \u00b6 Sample Request cat >spark-python-in-s3-ebs-static-localdir.json << EOF { \"name\": \"spark-python-in-s3-ebs-static-localdir\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.instances=10 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 \" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.node.selector.topology.kubernetes.io/zone\":\"<availability zone>\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.options.claimName\":\"ebs-static-pvc\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.readOnly\":\"false\", } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-ebs-static-localdir.json Observed Behavior: When the job gets started the driver pod and executor pods are scheduled only on those EKS worker nodes with the label topology.kubernetes.io/zone : . This ensures the spark job is run within a single AZ. If there are not enough resource within the AZ the pods will be in pending state until the Autoscaler(if configured) to kick in or more resources to be available. You can verify that the EBS volume created is mounted to driver pod and you can exec into the driver container to verify that the EBS volume is mounted. Also can verify from driver pod spec kubectl get pod <driver pod name> -n <namespace> -o yaml --export Dynamic Provisioning: \u00b6 EKS Admin Tasks: \u00b6 Create EBS Storage Class cat > demo - gp2 - sc . yaml << EOF apiVersion : storage . k8s . io / v1 kind : StorageClass metadata : name : demo - gp2 - sc provisioner : kubernetes . io / aws - ebs parameters : type : gp2 reclaimPolicy : Retain allowVolumeExpansion : true mountOptions : - debug volumeBindingMode : Immediate EOF kubectl apply - f demo - gp2 - sc . yaml create Persistent Volume for the EBS storage class - demo-gp2-sc cat > ebs - demo - gp2 - claim . yaml << EOF apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ebs - demo - gp2 - claim labels : app : chicago spec : storageClassName : demo - gp2 - sc accessModes : - ReadWriteOnce resources : requests : storage : 100 Gi EOF kubectl apply - f ebs - demo - gp2 - claim . yaml - n < namespace > Spark Developer Tasks: \u00b6 Sample Request cat >spark-python-in-s3-ebs-dynamic-localdir.json << EOF { \"name\": \"spark-python-in-s3-ebs-dynamic-localdir\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.instances=10 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.node.selector.topology.kubernetes.io/zone\":\"<availability zone>\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.options.claimName\":\"ebs-demo-gp2-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.readOnly\":\"false\", } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-ebs-dynamic-localdir.json Observed Behavior: When the job gets started the driver pod and executor pods are scheduled only on those EKS worker nodes with the label topology.kubernetes.io/zone : This ensures the spark job is run within a single AZ. If there are not enough resource within the AZ the pods will be in pending state until the Autoscaler(if configured) to kick in or more resources to be available. You can verify that the a EBS volume is provisioned dynamically by the EBS CSI driver and mounted to the driver pod. Also can verify from driver pod spec kubectl get pod <driver pod name> -n <namespace> -o yaml --export POINT TO NOTE: It is not possible to use this dynamic provisioning strategy for EBS to spark executors. Not possible to mount a new EBS volume to every Spark executor. Instead use a distributed file system like Lustre, EFS, NFS to mount to executors.","title":"EBS"},{"location":"storage/docs/spark/ebs/#mount-ebs-volume-to-spark-driver-and-executor-pods","text":"EBS Volumes can be mounted on spark driver and executor pods through static and dynamic provisioning - Difference between static and dynamic provisioning of Persistent Volumes: (Refer - https://kubernetes.io/docs/concepts/storage/persistent-volumes/#provisioning) EKS support for EBS CSI driver - https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html Documentation for EBS CSI driver - https://github.com/kubernetes-sigs/aws-ebs-csi-driver","title":"Mount EBS Volume to spark driver and executor pods"},{"location":"storage/docs/spark/ebs/#static-provisioning-","text":"","title":"Static Provisioning -"},{"location":"storage/docs/spark/ebs/#eks-admin-tasks","text":"Create EBS volumes aws ec2 --region <region> create-volume --availability-zone <availability zone> --size 50 { \"AvailabilityZone\": \"<availability zone>\", \"MultiAttachEnabled\": false, \"Tags\": [], \"Encrypted\": false, \"VolumeType\": \"gp2\", \"VolumeId\": \"<vol -id>\", \"State\": \"creating\", \"Iops\": 150, \"SnapshotId\": \"\", \"CreateTime\": \"2020-11-03T18:36:21.000Z\", \"Size\": 50 } Create Persistent Volume(PV) that has the EBS volume created above hardcoded cat > ebs - static - pv . yaml << EOF apiVersion : v1 kind : PersistentVolume metadata : name : ebs - static - pv spec : capacity : storage : 5 Gi accessModes : - ReadWriteOnce storageClassName : gp2 awsElasticBlockStore : fsType : ext4 volumeID : < vol - id > EOF kubectl apply - f ebs - static - pv . yaml - n < namespace > Create Persistent Volume Claim(PVC) for the Persistent Volume created above cat > ebs - static - pvc . yaml << EOF kind : PersistentVolumeClaim apiVersion : v1 metadata : name : ebs - static - pvc spec : accessModes : - ReadWriteOnce resources : requests : storage : 5 Gi volumeName : ebs - static - pv EOF kubectl apply - f ebs - static - pvc . yaml - n < namespace > PVC - ebs-static-pvc can be used by spark developer to mount to the spark pod NOTE: Pods running in EKS worker nodes can attach to only EBS volume provisioned in the same AZ as the EKS worker Node. Use node selector to schedule pods on a EKS worker node in a specified AZ","title":"EKS Admin Tasks:"},{"location":"storage/docs/spark/ebs/#spark-developer-tasks","text":"Sample Request cat >spark-python-in-s3-ebs-static-localdir.json << EOF { \"name\": \"spark-python-in-s3-ebs-static-localdir\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.instances=10 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 \" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.node.selector.topology.kubernetes.io/zone\":\"<availability zone>\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.options.claimName\":\"ebs-static-pvc\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.readOnly\":\"false\", } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-ebs-static-localdir.json Observed Behavior: When the job gets started the driver pod and executor pods are scheduled only on those EKS worker nodes with the label topology.kubernetes.io/zone : . This ensures the spark job is run within a single AZ. If there are not enough resource within the AZ the pods will be in pending state until the Autoscaler(if configured) to kick in or more resources to be available. You can verify that the EBS volume created is mounted to driver pod and you can exec into the driver container to verify that the EBS volume is mounted. Also can verify from driver pod spec kubectl get pod <driver pod name> -n <namespace> -o yaml --export","title":"Spark Developer Tasks:"},{"location":"storage/docs/spark/ebs/#dynamic-provisioning","text":"","title":"Dynamic Provisioning:"},{"location":"storage/docs/spark/ebs/#eks-admin-tasks_1","text":"Create EBS Storage Class cat > demo - gp2 - sc . yaml << EOF apiVersion : storage . k8s . io / v1 kind : StorageClass metadata : name : demo - gp2 - sc provisioner : kubernetes . io / aws - ebs parameters : type : gp2 reclaimPolicy : Retain allowVolumeExpansion : true mountOptions : - debug volumeBindingMode : Immediate EOF kubectl apply - f demo - gp2 - sc . yaml create Persistent Volume for the EBS storage class - demo-gp2-sc cat > ebs - demo - gp2 - claim . yaml << EOF apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ebs - demo - gp2 - claim labels : app : chicago spec : storageClassName : demo - gp2 - sc accessModes : - ReadWriteOnce resources : requests : storage : 100 Gi EOF kubectl apply - f ebs - demo - gp2 - claim . yaml - n < namespace >","title":"EKS Admin Tasks:"},{"location":"storage/docs/spark/ebs/#spark-developer-tasks_1","text":"Sample Request cat >spark-python-in-s3-ebs-dynamic-localdir.json << EOF { \"name\": \"spark-python-in-s3-ebs-dynamic-localdir\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.instances=10 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.node.selector.topology.kubernetes.io/zone\":\"<availability zone>\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.options.claimName\":\"ebs-demo-gp2-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.readOnly\":\"false\", } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-ebs-dynamic-localdir.json Observed Behavior: When the job gets started the driver pod and executor pods are scheduled only on those EKS worker nodes with the label topology.kubernetes.io/zone : This ensures the spark job is run within a single AZ. If there are not enough resource within the AZ the pods will be in pending state until the Autoscaler(if configured) to kick in or more resources to be available. You can verify that the a EBS volume is provisioned dynamically by the EBS CSI driver and mounted to the driver pod. Also can verify from driver pod spec kubectl get pod <driver pod name> -n <namespace> -o yaml --export POINT TO NOTE: It is not possible to use this dynamic provisioning strategy for EBS to spark executors. Not possible to mount a new EBS volume to every Spark executor. Instead use a distributed file system like Lustre, EFS, NFS to mount to executors.","title":"Spark Developer Tasks:"},{"location":"storage/docs/spark/fsx-lustre/","text":"EMR Containers integration with FSx for Lustre \u00b6 EMR Containers is a deployment option within EMR to run spark workloads on customer EKS clusters. EKS cluster provides the compute and ephemeral storage for the spark workloads. Ephemeral storage provided by EKS is carved from the EKS worker node disk storage and the lifecycle of the storage is bound by the lifecycle of the driver and executor pod. Need for durable storage: In cases where multiple spark applications are executed as part of a data pipeline, there are scenarios where data from one spark application is passed to subsequent spark applications - in this case data can be persisted in S3. Alternatively this data can also be persisted in FSx for Lustre - Lustre provides a fully managed, POSIX- compliant native filesystem interface for the data in s3, that can scale with compute provided by EKS. Storage is decoupled from compute and has its own lifecycle. Difference between static and dynamic provisioning of Persistent Volumes: (Refer - https://kubernetes.io/docs/concepts/storage/persistent-volumes/#provisioning) Data used in the below example is from https://registry.opendata.aws/nyc-tlc-trip-records-pds/ Static Provisioning - \u00b6 Provision a FSx for Lustre cluster: \u00b6 FSx for Lustre cluster can be provisioned through the AWS console at https://us-west-2.console.aws.amazon.com/fsx/home?region=us-west-2#file-systems FSx for Luster can also be provisioned through aws cli as below - https://docs.aws.amazon.com/cli/latest/reference/fsx/create-file-system.html How to decide what type of FSx for Lustre file system you need ? - https://docs.aws.amazon.com/fsx/latest/LustreGuide/LustreGuide.pdf Create a Security Group to attach to FSx for Lustre file system as below [Image: Screen Shot 2020-10-26 at 3.28.19 PM.png] Points to Note: Security group attached to the EKS worker nodes is given access on port number 988, 1021-1023 in inbound rules. Security group specified when creating the FSx for Lustre filesystem is given access on port number 988, 1021-1023 in inbound rules. Fsx for Lustre Provisioning through aws cli cat fsxLustreConfig.json << EOF { \"ClientRequestToken\": \"EMRContainers-fsxLustre-demo\", \"FileSystemType\": \"LUSTRE\", \"StorageCapacity\": 1200, \"StorageType\": \"SSD\", \"SubnetIds\": [ \"<subnet-id>\" ], \"SecurityGroupIds\": [ \"<securitygroup-id>\" ], \"LustreConfiguration\": { \"ImportPath\": \"s3://<s3 prefix>/\", \"ExportPath\": \"s3://<s3 prefix>/\", \"DeploymentType\": \"PERSISTENT_1\", \"AutoImportPolicy\": \"NEW_CHANGED\", \"PerUnitStorageThroughput\": 200 } } EOF Run the aws-cli to create the FSx for Lustre filesystem as below aws fsx create-file-system --cli-input-json file:///fsxLustreConfig.json Response is as below { \"FileSystem\": { \"VpcId\": \"<vpc id>\", \"Tags\": [], \"StorageType\": \"SSD\", \"SubnetIds\": [ \"<subnet-id>\" ], \"FileSystemType\": \"LUSTRE\", \"CreationTime\": 1603752401.183, \"ResourceARN\": \"<fsx resource arn>\", \"StorageCapacity\": 1200, \"LustreConfiguration\": { \"CopyTagsToBackups\": false, \"WeeklyMaintenanceStartTime\": \"7:11:30\", \"DataRepositoryConfiguration\": { \"ImportPath\": \"s3://<s3 prefix>\", \"AutoImportPolicy\": \"NEW_CHANGED\", \"ImportedFileChunkSize\": 1024, \"Lifecycle\": \"CREATING\", \"ExportPath\": \"s3://<s3 prefix>/\" }, \"DeploymentType\": \"PERSISTENT_1\", \"PerUnitStorageThroughput\": 200, \"MountName\": \"mvmxtbmv\" }, \"FileSystemId\": \"<filesystem id>\", \"DNSName\": \"<filesystem id>.fsx.<region>.amazonaws.com\", \"KmsKeyId\": \"arn:aws:kms:<region>:<account>:key/<key id>\", \"OwnerId\": \"<account>\", \"Lifecycle\": \"CREATING\" } } EKS admin tasks: \u00b6 (Refer - https://aws.amazon.com/blogs/opensource/using-fsx-lustre-csi-driver-amazon-eks/) Attach IAM policy to EKS worker node IAM role to enable access to FSx for Lustre - ReferDoc and Create a Security Group to\u2026 Install the FSx CSI Driver in EKS - Refer Doc Configure Storage Class for FSx for Lustre - Refer Doc Configure Persistent Volume and Persistent Volume Claim for FSx for Lustre FSx for Lustre file system is created as described above - Provision a FSx for Lustre cluster: Once provisioned, a persistent volume - as specified below is created with direct ( hard-coded) reference to the created lustre file system. Persistent Volume claim for this persistent volume will always use the same file system. cat >fsxLustre-static-pv.yaml <<EOF apiVersion: v1 kind: PersistentVolume metadata: name: fsx-pv spec: capacity: storage: 1200Gi volumeMode: Filesystem accessModes: - ReadWriteMany mountOptions: - flock persistentVolumeReclaimPolicy: Recycle csi: driver: fsx.csi.aws.com volumeHandle: <filesystem id> volumeAttributes: dnsname: <filesystem id>.fsx.<region>.amazonaws.com mountname: mvmxtbmv EOF kubectl apply -f fsxLustre-static-pv.yaml Now, a Persistent Volume Claim ( PVC) needs to be created that references PV created above. cat >fsxLustre-static-pvc.yaml <<EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: fsx-claim namespace: ns1 spec: accessModes: - ReadWriteMany storageClassName: \"\" resources: requests: storage: 1200Gi volumeName: fsx-pv EOF Spark Developer Tasks: \u00b6 Now spark applications can use fsx-claim in their spark application config to mount the FSx for Lustre filesystem to driver and executor container volumes. cat >spark-python-in-s3-fsx.json <<EOF { \"name\": \"spark-python-in-s3-fsx\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-repartition-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///Spark-Python-in-s3-fsx.json Expected Behavior: All spark jobs that are run with persistent volume claim as fsx-claim will mount to the statically created FSx for Lustre file system previously. Use case: A data pipeline consisting of 10 spark applications can all be mounted to the statically created FSx for Lustre file system and can write the intermediate output to a particular folder. The next spark job in the data pipeline that is dependent on this data can read from FSx for Lustre. Data that needs to be persisted beyond the scope of the data pipeline can be synced to S3 by creating data repository tasks - https://docs.aws.amazon.com/fsx/latest/LustreGuide/data-repository-tasks.html Data that is used often by multiple spark applications can also be stored in FSx for Lustre for improved performance. Dynamic Provisioning: \u00b6 There is no need to provision a FSx for Lustre file system in advance. Need to create a Storage-class resource that instantiates the Storage class for FSx for Lustre. PVC is created and it refers to the storage class resource that was created. Whenever a pod refers to the PVC, the storage class invokes the FSx for Lustre Container Storage Interface (CSI) to provision a Lustre file system on the fly dynamically. In this model - FSx for Lustre of type Scratch File Systems can be provisioned. - https://docs.aws.amazon.com/fsx/latest/LustreGuide/using-fsx-lustre.html EKS Admin Tasks: \u00b6 (Refer - https://aws.amazon.com/blogs/opensource/using-fsx-lustre-csi-driver-amazon-eks/) Attach IAM policy to EKS worker node IAM role to enable access to FSx for Lustre - Refer Doc and Create a Security Group to\u2026 Install the FSx CSI Driver in EKS - Refer Doc Configure Storage Class for FSx for Lustre - Refer Doc Configure Persistent Volume Claim( fsx-dynamic-claim ) for FSx for Lustre - Refer Doc cat >fsx-dynamic-claim.yaml <<EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: fsx-dynamic-claim spec: accessModes: - ReadWriteMany storageClassName: fsx-sc resources: requests: storage: 3600Gi EOF kubectl apply -f fsx-dynamic-pvc.yaml -n <namespace> Spark Developer Tasks: \u00b6 cat >spark-python-in-s3-fsx-dynamic.json << EOF { \"name\": \"spark-python-in-s3-fsx-dynamic\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-repartition-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.kubernetes.pyspark.pythonVersion=3 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 --conf spark.sql.shuffle.partitions=1000\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.local.dir\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.options.claimName\":\"fsx-dynamic-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///Spark-Python-in-s3-fsx-dynamic.json Expected Result: Statically provisioned FSx for Lustre is mounted to /var/data/ as before. For all the executors we provision a SCRATCH 1 deployment type FSx for Lustre is provisioned on the fly dynamically by the Storage class that was created. There will be a latency before the first executor can start running - because the Lustre has to be created. Once it is created the same Lustre instance is mounted to all the executor. Also note - \"spark.local.dir\":\"/var/spark/spill/\" is used to force executor to use this folder mounted to Lustre for all spill and shuffle data. Once thePod is terminated the Lustre file system is deleted or retained based on the configuration. Also this dynamically created Lustre file system can also be linked to a S3 path like the statically created filesystem. References: https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi.html","title":"FSx for Lustre"},{"location":"storage/docs/spark/fsx-lustre/#emr-containers-integration-with-fsx-for-lustre","text":"EMR Containers is a deployment option within EMR to run spark workloads on customer EKS clusters. EKS cluster provides the compute and ephemeral storage for the spark workloads. Ephemeral storage provided by EKS is carved from the EKS worker node disk storage and the lifecycle of the storage is bound by the lifecycle of the driver and executor pod. Need for durable storage: In cases where multiple spark applications are executed as part of a data pipeline, there are scenarios where data from one spark application is passed to subsequent spark applications - in this case data can be persisted in S3. Alternatively this data can also be persisted in FSx for Lustre - Lustre provides a fully managed, POSIX- compliant native filesystem interface for the data in s3, that can scale with compute provided by EKS. Storage is decoupled from compute and has its own lifecycle. Difference between static and dynamic provisioning of Persistent Volumes: (Refer - https://kubernetes.io/docs/concepts/storage/persistent-volumes/#provisioning) Data used in the below example is from https://registry.opendata.aws/nyc-tlc-trip-records-pds/","title":"EMR Containers integration with FSx for Lustre"},{"location":"storage/docs/spark/fsx-lustre/#static-provisioning-","text":"","title":"Static Provisioning -"},{"location":"storage/docs/spark/fsx-lustre/#provision-a-fsx-for-lustre-cluster","text":"FSx for Lustre cluster can be provisioned through the AWS console at https://us-west-2.console.aws.amazon.com/fsx/home?region=us-west-2#file-systems FSx for Luster can also be provisioned through aws cli as below - https://docs.aws.amazon.com/cli/latest/reference/fsx/create-file-system.html How to decide what type of FSx for Lustre file system you need ? - https://docs.aws.amazon.com/fsx/latest/LustreGuide/LustreGuide.pdf Create a Security Group to attach to FSx for Lustre file system as below [Image: Screen Shot 2020-10-26 at 3.28.19 PM.png] Points to Note: Security group attached to the EKS worker nodes is given access on port number 988, 1021-1023 in inbound rules. Security group specified when creating the FSx for Lustre filesystem is given access on port number 988, 1021-1023 in inbound rules. Fsx for Lustre Provisioning through aws cli cat fsxLustreConfig.json << EOF { \"ClientRequestToken\": \"EMRContainers-fsxLustre-demo\", \"FileSystemType\": \"LUSTRE\", \"StorageCapacity\": 1200, \"StorageType\": \"SSD\", \"SubnetIds\": [ \"<subnet-id>\" ], \"SecurityGroupIds\": [ \"<securitygroup-id>\" ], \"LustreConfiguration\": { \"ImportPath\": \"s3://<s3 prefix>/\", \"ExportPath\": \"s3://<s3 prefix>/\", \"DeploymentType\": \"PERSISTENT_1\", \"AutoImportPolicy\": \"NEW_CHANGED\", \"PerUnitStorageThroughput\": 200 } } EOF Run the aws-cli to create the FSx for Lustre filesystem as below aws fsx create-file-system --cli-input-json file:///fsxLustreConfig.json Response is as below { \"FileSystem\": { \"VpcId\": \"<vpc id>\", \"Tags\": [], \"StorageType\": \"SSD\", \"SubnetIds\": [ \"<subnet-id>\" ], \"FileSystemType\": \"LUSTRE\", \"CreationTime\": 1603752401.183, \"ResourceARN\": \"<fsx resource arn>\", \"StorageCapacity\": 1200, \"LustreConfiguration\": { \"CopyTagsToBackups\": false, \"WeeklyMaintenanceStartTime\": \"7:11:30\", \"DataRepositoryConfiguration\": { \"ImportPath\": \"s3://<s3 prefix>\", \"AutoImportPolicy\": \"NEW_CHANGED\", \"ImportedFileChunkSize\": 1024, \"Lifecycle\": \"CREATING\", \"ExportPath\": \"s3://<s3 prefix>/\" }, \"DeploymentType\": \"PERSISTENT_1\", \"PerUnitStorageThroughput\": 200, \"MountName\": \"mvmxtbmv\" }, \"FileSystemId\": \"<filesystem id>\", \"DNSName\": \"<filesystem id>.fsx.<region>.amazonaws.com\", \"KmsKeyId\": \"arn:aws:kms:<region>:<account>:key/<key id>\", \"OwnerId\": \"<account>\", \"Lifecycle\": \"CREATING\" } }","title":"Provision a FSx for Lustre cluster:"},{"location":"storage/docs/spark/fsx-lustre/#eks-admin-tasks","text":"(Refer - https://aws.amazon.com/blogs/opensource/using-fsx-lustre-csi-driver-amazon-eks/) Attach IAM policy to EKS worker node IAM role to enable access to FSx for Lustre - ReferDoc and Create a Security Group to\u2026 Install the FSx CSI Driver in EKS - Refer Doc Configure Storage Class for FSx for Lustre - Refer Doc Configure Persistent Volume and Persistent Volume Claim for FSx for Lustre FSx for Lustre file system is created as described above - Provision a FSx for Lustre cluster: Once provisioned, a persistent volume - as specified below is created with direct ( hard-coded) reference to the created lustre file system. Persistent Volume claim for this persistent volume will always use the same file system. cat >fsxLustre-static-pv.yaml <<EOF apiVersion: v1 kind: PersistentVolume metadata: name: fsx-pv spec: capacity: storage: 1200Gi volumeMode: Filesystem accessModes: - ReadWriteMany mountOptions: - flock persistentVolumeReclaimPolicy: Recycle csi: driver: fsx.csi.aws.com volumeHandle: <filesystem id> volumeAttributes: dnsname: <filesystem id>.fsx.<region>.amazonaws.com mountname: mvmxtbmv EOF kubectl apply -f fsxLustre-static-pv.yaml Now, a Persistent Volume Claim ( PVC) needs to be created that references PV created above. cat >fsxLustre-static-pvc.yaml <<EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: fsx-claim namespace: ns1 spec: accessModes: - ReadWriteMany storageClassName: \"\" resources: requests: storage: 1200Gi volumeName: fsx-pv EOF","title":"EKS admin tasks:"},{"location":"storage/docs/spark/fsx-lustre/#spark-developer-tasks","text":"Now spark applications can use fsx-claim in their spark application config to mount the FSx for Lustre filesystem to driver and executor container volumes. cat >spark-python-in-s3-fsx.json <<EOF { \"name\": \"spark-python-in-s3-fsx\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-repartition-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///Spark-Python-in-s3-fsx.json Expected Behavior: All spark jobs that are run with persistent volume claim as fsx-claim will mount to the statically created FSx for Lustre file system previously. Use case: A data pipeline consisting of 10 spark applications can all be mounted to the statically created FSx for Lustre file system and can write the intermediate output to a particular folder. The next spark job in the data pipeline that is dependent on this data can read from FSx for Lustre. Data that needs to be persisted beyond the scope of the data pipeline can be synced to S3 by creating data repository tasks - https://docs.aws.amazon.com/fsx/latest/LustreGuide/data-repository-tasks.html Data that is used often by multiple spark applications can also be stored in FSx for Lustre for improved performance.","title":"Spark Developer Tasks:"},{"location":"storage/docs/spark/fsx-lustre/#dynamic-provisioning","text":"There is no need to provision a FSx for Lustre file system in advance. Need to create a Storage-class resource that instantiates the Storage class for FSx for Lustre. PVC is created and it refers to the storage class resource that was created. Whenever a pod refers to the PVC, the storage class invokes the FSx for Lustre Container Storage Interface (CSI) to provision a Lustre file system on the fly dynamically. In this model - FSx for Lustre of type Scratch File Systems can be provisioned. - https://docs.aws.amazon.com/fsx/latest/LustreGuide/using-fsx-lustre.html","title":"Dynamic Provisioning:"},{"location":"storage/docs/spark/fsx-lustre/#eks-admin-tasks_1","text":"(Refer - https://aws.amazon.com/blogs/opensource/using-fsx-lustre-csi-driver-amazon-eks/) Attach IAM policy to EKS worker node IAM role to enable access to FSx for Lustre - Refer Doc and Create a Security Group to\u2026 Install the FSx CSI Driver in EKS - Refer Doc Configure Storage Class for FSx for Lustre - Refer Doc Configure Persistent Volume Claim( fsx-dynamic-claim ) for FSx for Lustre - Refer Doc cat >fsx-dynamic-claim.yaml <<EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: fsx-dynamic-claim spec: accessModes: - ReadWriteMany storageClassName: fsx-sc resources: requests: storage: 3600Gi EOF kubectl apply -f fsx-dynamic-pvc.yaml -n <namespace>","title":"EKS Admin Tasks:"},{"location":"storage/docs/spark/fsx-lustre/#spark-developer-tasks_1","text":"cat >spark-python-in-s3-fsx-dynamic.json << EOF { \"name\": \"spark-python-in-s3-fsx-dynamic\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-repartition-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.kubernetes.pyspark.pythonVersion=3 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 --conf spark.sql.shuffle.partitions=1000\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.local.dir\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.options.claimName\":\"fsx-dynamic-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///Spark-Python-in-s3-fsx-dynamic.json Expected Result: Statically provisioned FSx for Lustre is mounted to /var/data/ as before. For all the executors we provision a SCRATCH 1 deployment type FSx for Lustre is provisioned on the fly dynamically by the Storage class that was created. There will be a latency before the first executor can start running - because the Lustre has to be created. Once it is created the same Lustre instance is mounted to all the executor. Also note - \"spark.local.dir\":\"/var/spark/spill/\" is used to force executor to use this folder mounted to Lustre for all spill and shuffle data. Once thePod is terminated the Lustre file system is deleted or retained based on the configuration. Also this dynamically created Lustre file system can also be linked to a S3 path like the statically created filesystem. References: https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi.html","title":"Spark Developer Tasks:"},{"location":"submit-applications/docs/spark/","text":"","title":"Home"},{"location":"submit-applications/docs/spark/java-and-scala/","text":"","title":"Java and scala"},{"location":"submit-applications/docs/spark/pyspark/","text":"Pyspark Job submission \u00b6 Python interpreter is bundled in the EMR containers spark image that is used to run the spark job. Python code self contained in a single .py file \u00b6 To start with, in the most simplest scenario - the example below shows how to submit a pi.py file that is self contained and doesn't need any other dependencies. In this example pi.py is part of the spark image and is passed to the start-job-run command using local:// path prefix. cat > spark - python - in - image . json << EOF { \"name\" : \"spark-python-in-image\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"local:///usr/lib/spark/examples/src/main/python/pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-image.json In the below example - pi.py is placed in a mounted volume. FSx for Lustre is mounted as a Persistent Volume on the driver pod under /var/data/ and will be referenced by local:// file prefix. For more information on how to mount FSx for lustre - refer - EMR-Containers-integration-with-FSx-for-Lustre This approach can be used to reference spark code and dependencies from remote locations if s3 access from driver and executor pods is not desired cat > spark - python - in - FSx . json << EOF { \"name\" : \"spark-python-in-FSx\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"local:///var/data/FSxLustre-pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-Fsx.json Python code with dependencies bundled and passed in through \u2014py-files spark configuration \u00b6 Refer - https://spark.apache.org/docs/latest/submitting-applications.html All dependencies for the pyspark code can be passed in the below ways comma separated list of .py files Bundled as a zip file Bundled as a .egg file Bundled as a .whl file comma separated list of .py files \u00b6 This is not a scalable approach as the number of dependent files can grow to a large number, and also need to manually specify any of transitive dependencies. cat > py - files - pi . py << EOF from __future__ import print_function import sys from random import random from operator import add from pyspark.sql import SparkSession from pyspark import SparkContext import dependentFunc if __name__ == \"__main__\" : \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext partitions = int ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else 2 n = 100000 * partitions def f ( _ ): x = random () * 2 - 1 y = random () * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark . sparkContext . parallelize ( range ( 1 , n + 1 ), partitions ) . map ( f ) . reduce ( add ) dependentFunc . message () print ( \"Pi is roughly %f \" % ( 4.0 * count / n )) spark . stop () EOF cat > dependentFunc . py << EOF def message (): print ( \"Printing from inside the dependent python file\" ) EOF Upload dependentFunc.py and py-files-pi.py to s3 Request cat > spark - python - in - s3 - dependency - files << EOF { \"name\" : \"spark-python-in-s3-dependency-files\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/dependentFunc.py --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-files.json Bundled as a zip file \u00b6 Points to Note: Each dependency folder should have init .py file - https://docs.python.org/3/reference/import.html#regular-packages Zip should be done at the top folder level and using the -r option for all dependency folders. dependentFunc.py from earlier example has been bundled into a zip with package folders and attached below. zip -r pyspark-packaged-dependency-src.zip . adding: dependent/ (stored 0%) adding: dependent/__init__.py (stored 0%) adding: dependent/dependentFunc.py (deflated 7%) pyspark-packaged-dependency-src.zip - Place this file in a s3 location cat > py - files - zip - pi . py << EOF from __future__ import print_function import sys from random import random from operator import add from pyspark.sql import SparkSession from pyspark import SparkContext ** from dependent import dependentFunc ** if __name__ == \"__main__\" : \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext partitions = int ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else 2 n = 100000 * partitions def f ( _ ): x = random () * 2 - 1 y = random () * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark . sparkContext . parallelize ( range ( 1 , n + 1 ), partitions ) . map ( f ) . reduce ( add ) dependentFunc . message () print ( \"Pi is roughly %f \" % ( 4.0 * count / n )) spark . stop () EOF Request cat > spark - python - in - s3 - dependency - zip . json << EOF { \"name\" : \"spark-python-in-s3-dependency-zip\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark-packaged-dependency-src.zip --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-zip.json Bundled as a .egg file \u00b6 Create a folder structure as in the below screenshot with the code from the previous example - py-files-zip-pi.py, dependentFunc.py Steps to create .egg file cd /pyspark-packaged-example pip install setuptools python setup.py bdist_egg Copy dist/pyspark_packaged_example-0.0.3-py3.8.egg to a s3 location Request cat > spark - python - in - s3 - dependency - egg . json << EOF { \"name\" : \"spark-python-in-s3-dependency-egg\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark_packaged_example-0.0.3-py3.8.egg --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-egg.json Bundled as a .whl file \u00b6 Create a folder structure as in the below screenshot with the code from the previous example - py-files-zip-pi.py, dependentFunc.py [Image: Screen Shot 2020-11-16 at 3.34.12 PM.png]Steps to create .egg file cd /pyspark-packaged-example `pip install wheel` python setup.py bdist_wheel Copy dist/ pyspark_packaged_example-0.0.3-py3-none-any.whl to a s3 location Request cat > spark - python - in - s3 - dependency - wheel . json << EOF { \"name\" : \"spark-python-in-s3-dependency-wheel\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark_packaged_example-0.0.3-py3-none-any.whl --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-wheel.json Python code with dependencies bundled - \"spark.pyspark.virtualenv.enabled\":\"true\" \u00b6 This will not work - this feature only works with YARN - cluster mode In this implementation for YARN - the dependencies will be installed from the repository for every driver and executor. This might not be a more scalable model as per https://issues.apache.org/jira/browse/SPARK-25433. Recommended solution is to pass in the dependencies as PEX file. Python code with dependencies bundled as a PEX file \u00b6 `docker run ``-``it ``-``v $``(``pwd``):``/workdir python:3.7.9-buster /``bin``/``bash ``#python 3.7.9 is installed in EMR 6.1.0` `pip3 install pex` `pex ``--``python``=``python3`` ``--``inherit``-``path``=prefer`` ``-``v numpy ``-``o numpy_dep.pex` For the commands used above - Refer - https://github.com/pantsbuild/pex https://readthedocs.org/projects/manypex/downloads/pdf/latest/ http://www.legendu.net/misc/blog/tips-on-pex/ http://www.legendu.net/misc/blog/packaging-python-dependencies-for-pyspark-using-pex/ Place numpy_dep.pex in a s3 location that is mapped to a FSx for Lustre cluster. numpy_dep.pex can be placed on any Kubernetes persistent volume and mounted to the driver pod and executor pod. Request cat > spark - python - in - s3 - pex - fsx . json << EOF { \"name\" : \"spark-python-in-s3-pex-fsx\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/kmeans.py\" , \"entryPointArguments\" : [ \"s3://<s3 prefix>/kmeans_data.txt\" , \"2\" , \"3\" ], \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.executor.instances\" : \"3\" , \"spark.dynamicAllocation.enabled\" : \"false\" , \"spark.kubernetes.pyspark.pythonVersion\" : \"3\" , \"spark.kubernetes.driverEnv.PEX_ROOT\" : \"./tmp\" , \"spark.executorEnv.PEX_ROOT\" : \"./tmp\" , \"spark.kubernetes.driverEnv.PEX_INHERIT_PATH\" : \"prefer\" , \"spark.executorEnv.PEX_INHERIT_PATH\" : \"prefer\" , \"spark.kubernetes.driverEnv.PEX_VERBOSE\" : \"10\" , \"spark.kubernetes.driverEnv.PEX_PYTHON\" : \"python3\" , \"spark.executorEnv.PEX_PYTHON\" : \"python3\" , \"spark.pyspark.driver.python\" : \"/var/data/numpy_dep.pex\" , \"spark.pyspark.python\" : \"/var/data/numpy_dep.pex\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } aws emr - containers start - job - run --cli-input-json file:////Spark-Python-in-s3-pex-fsx.json Point to Note: PEX files don\u2019t have the python interpreter bundled with it. Using the PEX env variables we pass in the python interpreter installed in the spark driver and executor docker image. Conda-pack has the python interpreter bundled in the package. pex vs conda-pack A pex file contain only Python packages but not a Python interpreter in it while a conda-pack environment has a Python interpreter as well, so with the same Python packages a conda-pack environment is much larger than a pex file. A conda-pack environment is a tar.gz file and need to be decompressed before being used while a pex file can be used directly. If a Python interpreter exists, pex is a better option than conda-pack. However, conda-pack is the ONLY CHOICE if you need a specific version of Python interpreter which does not exist and you do not have permission to install one (e.g., when you need to use a specific version of Python interpreter with an enterprise PySpark cluster). If the pex file or conda-pack environment needs to be distributed to machines on demand, there are some overhead before running your application. With the same Python packages, a conda-pack environment has large overhead/latency than the pex file as the conda-pack environment is usually much larger and need to be decompressed before being used. For more information - refer http://www.legendu.net/misc/blog/tips-on-pex/ Python code with dependencies bundled as a tar.gz file with conda-pack \u00b6 Refer - https://conda.github.io/conda-pack/spark.html Install conda through Miniconda - https://conda.io/miniconda.html Open a new terminal and execute the below commands conda create -y -n example python=3.5 numpy conda activate example pip install conda-pack conda pack -f -o numpy_environment.tar.gz Place numpy_environment.tar.gz in a s3 location that is mapped to a FSx for Lustre cluster. numpy_dep.pex can be placed on any Kubernetes persistent volume and mounted to the driver pod and executor pod. Request { \"name\": \"spark-python-in-s3-conda-fsx\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/kmeans.py\", \"entryPointArguments\": [ \"s3://<s3 prefix>/kmeans_data.txt\", \"2\", \"3\" ], \"sparkSubmitParameters\": \"--verbose --archives /var/data/numpy_environment.tar.gz#environment --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.instances\": \"3\", \"spark.dynamicAllocation.enabled\":\"false\", \"spark.files\":\"/var/data/numpy_environment.tar.gz#environment\", \"spark.kubernetes.pyspark.pythonVersion\":\"3\", \"spark.pyspark.driver.python\":\"./environment/bin/python\", \"spark.pyspark.python\":\"./environment/bin/python\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } The above request doesn't WORK with spark on kubernetes","title":"Pyspark"},{"location":"submit-applications/docs/spark/pyspark/#pyspark-job-submission","text":"Python interpreter is bundled in the EMR containers spark image that is used to run the spark job.","title":"Pyspark Job submission"},{"location":"submit-applications/docs/spark/pyspark/#python-code-self-contained-in-a-single-py-file","text":"To start with, in the most simplest scenario - the example below shows how to submit a pi.py file that is self contained and doesn't need any other dependencies. In this example pi.py is part of the spark image and is passed to the start-job-run command using local:// path prefix. cat > spark - python - in - image . json << EOF { \"name\" : \"spark-python-in-image\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"local:///usr/lib/spark/examples/src/main/python/pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-image.json In the below example - pi.py is placed in a mounted volume. FSx for Lustre is mounted as a Persistent Volume on the driver pod under /var/data/ and will be referenced by local:// file prefix. For more information on how to mount FSx for lustre - refer - EMR-Containers-integration-with-FSx-for-Lustre This approach can be used to reference spark code and dependencies from remote locations if s3 access from driver and executor pods is not desired cat > spark - python - in - FSx . json << EOF { \"name\" : \"spark-python-in-FSx\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"local:///var/data/FSxLustre-pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-Fsx.json","title":"Python code self contained in a single .py file"},{"location":"submit-applications/docs/spark/pyspark/#python-code-with-dependencies-bundled-and-passed-in-through-py-files-spark-configuration","text":"Refer - https://spark.apache.org/docs/latest/submitting-applications.html All dependencies for the pyspark code can be passed in the below ways comma separated list of .py files Bundled as a zip file Bundled as a .egg file Bundled as a .whl file","title":"Python code with dependencies bundled and passed in through \u2014py-files spark configuration"},{"location":"submit-applications/docs/spark/pyspark/#comma-separated-list-of-py-files","text":"This is not a scalable approach as the number of dependent files can grow to a large number, and also need to manually specify any of transitive dependencies. cat > py - files - pi . py << EOF from __future__ import print_function import sys from random import random from operator import add from pyspark.sql import SparkSession from pyspark import SparkContext import dependentFunc if __name__ == \"__main__\" : \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext partitions = int ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else 2 n = 100000 * partitions def f ( _ ): x = random () * 2 - 1 y = random () * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark . sparkContext . parallelize ( range ( 1 , n + 1 ), partitions ) . map ( f ) . reduce ( add ) dependentFunc . message () print ( \"Pi is roughly %f \" % ( 4.0 * count / n )) spark . stop () EOF cat > dependentFunc . py << EOF def message (): print ( \"Printing from inside the dependent python file\" ) EOF Upload dependentFunc.py and py-files-pi.py to s3 Request cat > spark - python - in - s3 - dependency - files << EOF { \"name\" : \"spark-python-in-s3-dependency-files\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/dependentFunc.py --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-files.json","title":"comma separated list of .py files"},{"location":"submit-applications/docs/spark/pyspark/#bundled-as-a-zip-file","text":"Points to Note: Each dependency folder should have init .py file - https://docs.python.org/3/reference/import.html#regular-packages Zip should be done at the top folder level and using the -r option for all dependency folders. dependentFunc.py from earlier example has been bundled into a zip with package folders and attached below. zip -r pyspark-packaged-dependency-src.zip . adding: dependent/ (stored 0%) adding: dependent/__init__.py (stored 0%) adding: dependent/dependentFunc.py (deflated 7%) pyspark-packaged-dependency-src.zip - Place this file in a s3 location cat > py - files - zip - pi . py << EOF from __future__ import print_function import sys from random import random from operator import add from pyspark.sql import SparkSession from pyspark import SparkContext ** from dependent import dependentFunc ** if __name__ == \"__main__\" : \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext partitions = int ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else 2 n = 100000 * partitions def f ( _ ): x = random () * 2 - 1 y = random () * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark . sparkContext . parallelize ( range ( 1 , n + 1 ), partitions ) . map ( f ) . reduce ( add ) dependentFunc . message () print ( \"Pi is roughly %f \" % ( 4.0 * count / n )) spark . stop () EOF Request cat > spark - python - in - s3 - dependency - zip . json << EOF { \"name\" : \"spark-python-in-s3-dependency-zip\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark-packaged-dependency-src.zip --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-zip.json","title":"Bundled as a zip file"},{"location":"submit-applications/docs/spark/pyspark/#bundled-as-a-egg-file","text":"Create a folder structure as in the below screenshot with the code from the previous example - py-files-zip-pi.py, dependentFunc.py Steps to create .egg file cd /pyspark-packaged-example pip install setuptools python setup.py bdist_egg Copy dist/pyspark_packaged_example-0.0.3-py3.8.egg to a s3 location Request cat > spark - python - in - s3 - dependency - egg . json << EOF { \"name\" : \"spark-python-in-s3-dependency-egg\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark_packaged_example-0.0.3-py3.8.egg --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-egg.json","title":"Bundled as a .egg file"},{"location":"submit-applications/docs/spark/pyspark/#bundled-as-a-whl-file","text":"Create a folder structure as in the below screenshot with the code from the previous example - py-files-zip-pi.py, dependentFunc.py [Image: Screen Shot 2020-11-16 at 3.34.12 PM.png]Steps to create .egg file cd /pyspark-packaged-example `pip install wheel` python setup.py bdist_wheel Copy dist/ pyspark_packaged_example-0.0.3-py3-none-any.whl to a s3 location Request cat > spark - python - in - s3 - dependency - wheel . json << EOF { \"name\" : \"spark-python-in-s3-dependency-wheel\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark_packaged_example-0.0.3-py3-none-any.whl --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-wheel.json","title":"Bundled as a .whl file"},{"location":"submit-applications/docs/spark/pyspark/#python-code-with-dependencies-bundled-sparkpysparkvirtualenvenabledtrue","text":"This will not work - this feature only works with YARN - cluster mode In this implementation for YARN - the dependencies will be installed from the repository for every driver and executor. This might not be a more scalable model as per https://issues.apache.org/jira/browse/SPARK-25433. Recommended solution is to pass in the dependencies as PEX file.","title":"Python code with dependencies bundled - \"spark.pyspark.virtualenv.enabled\":\"true\""},{"location":"submit-applications/docs/spark/pyspark/#python-code-with-dependencies-bundled-as-a-pex-file","text":"`docker run ``-``it ``-``v $``(``pwd``):``/workdir python:3.7.9-buster /``bin``/``bash ``#python 3.7.9 is installed in EMR 6.1.0` `pip3 install pex` `pex ``--``python``=``python3`` ``--``inherit``-``path``=prefer`` ``-``v numpy ``-``o numpy_dep.pex` For the commands used above - Refer - https://github.com/pantsbuild/pex https://readthedocs.org/projects/manypex/downloads/pdf/latest/ http://www.legendu.net/misc/blog/tips-on-pex/ http://www.legendu.net/misc/blog/packaging-python-dependencies-for-pyspark-using-pex/ Place numpy_dep.pex in a s3 location that is mapped to a FSx for Lustre cluster. numpy_dep.pex can be placed on any Kubernetes persistent volume and mounted to the driver pod and executor pod. Request cat > spark - python - in - s3 - pex - fsx . json << EOF { \"name\" : \"spark-python-in-s3-pex-fsx\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/kmeans.py\" , \"entryPointArguments\" : [ \"s3://<s3 prefix>/kmeans_data.txt\" , \"2\" , \"3\" ], \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.executor.instances\" : \"3\" , \"spark.dynamicAllocation.enabled\" : \"false\" , \"spark.kubernetes.pyspark.pythonVersion\" : \"3\" , \"spark.kubernetes.driverEnv.PEX_ROOT\" : \"./tmp\" , \"spark.executorEnv.PEX_ROOT\" : \"./tmp\" , \"spark.kubernetes.driverEnv.PEX_INHERIT_PATH\" : \"prefer\" , \"spark.executorEnv.PEX_INHERIT_PATH\" : \"prefer\" , \"spark.kubernetes.driverEnv.PEX_VERBOSE\" : \"10\" , \"spark.kubernetes.driverEnv.PEX_PYTHON\" : \"python3\" , \"spark.executorEnv.PEX_PYTHON\" : \"python3\" , \"spark.pyspark.driver.python\" : \"/var/data/numpy_dep.pex\" , \"spark.pyspark.python\" : \"/var/data/numpy_dep.pex\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } aws emr - containers start - job - run --cli-input-json file:////Spark-Python-in-s3-pex-fsx.json Point to Note: PEX files don\u2019t have the python interpreter bundled with it. Using the PEX env variables we pass in the python interpreter installed in the spark driver and executor docker image. Conda-pack has the python interpreter bundled in the package. pex vs conda-pack A pex file contain only Python packages but not a Python interpreter in it while a conda-pack environment has a Python interpreter as well, so with the same Python packages a conda-pack environment is much larger than a pex file. A conda-pack environment is a tar.gz file and need to be decompressed before being used while a pex file can be used directly. If a Python interpreter exists, pex is a better option than conda-pack. However, conda-pack is the ONLY CHOICE if you need a specific version of Python interpreter which does not exist and you do not have permission to install one (e.g., when you need to use a specific version of Python interpreter with an enterprise PySpark cluster). If the pex file or conda-pack environment needs to be distributed to machines on demand, there are some overhead before running your application. With the same Python packages, a conda-pack environment has large overhead/latency than the pex file as the conda-pack environment is usually much larger and need to be decompressed before being used. For more information - refer http://www.legendu.net/misc/blog/tips-on-pex/","title":"Python code with dependencies bundled as a PEX file"},{"location":"submit-applications/docs/spark/pyspark/#python-code-with-dependencies-bundled-as-a-targz-file-with-conda-pack","text":"Refer - https://conda.github.io/conda-pack/spark.html Install conda through Miniconda - https://conda.io/miniconda.html Open a new terminal and execute the below commands conda create -y -n example python=3.5 numpy conda activate example pip install conda-pack conda pack -f -o numpy_environment.tar.gz Place numpy_environment.tar.gz in a s3 location that is mapped to a FSx for Lustre cluster. numpy_dep.pex can be placed on any Kubernetes persistent volume and mounted to the driver pod and executor pod. Request { \"name\": \"spark-python-in-s3-conda-fsx\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/kmeans.py\", \"entryPointArguments\": [ \"s3://<s3 prefix>/kmeans_data.txt\", \"2\", \"3\" ], \"sparkSubmitParameters\": \"--verbose --archives /var/data/numpy_environment.tar.gz#environment --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.instances\": \"3\", \"spark.dynamicAllocation.enabled\":\"false\", \"spark.files\":\"/var/data/numpy_environment.tar.gz#environment\", \"spark.kubernetes.pyspark.pythonVersion\":\"3\", \"spark.pyspark.driver.python\":\"./environment/bin/python\", \"spark.pyspark.python\":\"./environment/bin/python\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } The above request doesn't WORK with spark on kubernetes","title":"Python code with dependencies bundled as a tar.gz file with conda-pack"},{"location":"submit-applications/docs/spark/sparkr/","text":"","title":"Sparkr"},{"location":"submit-applications/docs/spark/sparksql/","text":"","title":"Sparksql"}]}