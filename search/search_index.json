{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the EMR Containers Best Practices Guide. The primary goal of this project is to offer a set of best practices and templates to get started with Amazon EMR on EKS . We elected to publish this guidance to GitHub so we could iterate quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community. Contributing \u00b6 We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Introduction"},{"location":"#contributing","text":"We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Contributing"},{"location":"debugging/docs/","text":"","title":"Index"},{"location":"debugging/docs/change-log-level/","text":"Change Log level for Spark application on EMR on EKS \u00b6 To obtain more detail about their application or job submission, Spark application developers can change the log level of their job to different levels depending on their requirements. Spark uses apache log4j for logging. Change log level to DEBUG \u00b6 Using EMR classification \u00b6 Log level of spark applications can be changed using the EMR spark-log4j configuration classification. Request pi.py used in the below request payload is from spark examples spark-log4j classification can be used to configure values in log4j.properties cat > Spark - Python - in - s3 - debug - log . json << EOF { \"name\" : \"spark-python-in-s3-debug-log-classification\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } , { \"classification\" : \"spark-log4j\" , \"properties\" : { \"log4j.rootCategory\" : \"DEBUG, console\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-debug-log.json The above request will print DEBUG logs in the spark driver and executor containers. The generated logs will be pushed to S3 and AWS Cloudwatch logs as configured in the request. Custom log4j properties \u00b6 Download log4j properties from here . Edit log4j.properties with log level as required. Save the edited log4j.properties in a mounted volume. In this example log4j.properties is placed in a s3 bucket that is mapped to a FSx for Lustre filesystem . Request pi.py used in the below request payload is from spark examples cat > Spark - Python - in - s3 - debug - log . json << EOF { \"name\" : \"spark-python-in-s3-debug-log\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.driver.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.executor.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-debug-log.json Configurations of interest: Below configuration enables spark driver and executor to pickup the log4j configuration file from /var/data/ folder mounted to the driver and executor containers. For guide to mount FSx for Lustre to driver and executor containers - refer to EMR Containers integration with FSx for Lustre \"spark.driver.extraJavaOptions\":\"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\", \"spark.executor.extraJavaOptions\":\"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\",","title":"Change Log Level"},{"location":"debugging/docs/change-log-level/#change-log-level-for-spark-application-on-emr-on-eks","text":"To obtain more detail about their application or job submission, Spark application developers can change the log level of their job to different levels depending on their requirements. Spark uses apache log4j for logging.","title":"Change Log level for Spark application on EMR on EKS"},{"location":"debugging/docs/change-log-level/#change-log-level-to-debug","text":"","title":"Change log level to DEBUG"},{"location":"debugging/docs/change-log-level/#using-emr-classification","text":"Log level of spark applications can be changed using the EMR spark-log4j configuration classification. Request pi.py used in the below request payload is from spark examples spark-log4j classification can be used to configure values in log4j.properties cat > Spark - Python - in - s3 - debug - log . json << EOF { \"name\" : \"spark-python-in-s3-debug-log-classification\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } , { \"classification\" : \"spark-log4j\" , \"properties\" : { \"log4j.rootCategory\" : \"DEBUG, console\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-debug-log.json The above request will print DEBUG logs in the spark driver and executor containers. The generated logs will be pushed to S3 and AWS Cloudwatch logs as configured in the request.","title":"Using EMR classification"},{"location":"debugging/docs/change-log-level/#custom-log4j-properties","text":"Download log4j properties from here . Edit log4j.properties with log level as required. Save the edited log4j.properties in a mounted volume. In this example log4j.properties is placed in a s3 bucket that is mapped to a FSx for Lustre filesystem . Request pi.py used in the below request payload is from spark examples cat > Spark - Python - in - s3 - debug - log . json << EOF { \"name\" : \"spark-python-in-s3-debug-log\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.driver.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.executor.extraJavaOptions\" : \"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-debug-log.json Configurations of interest: Below configuration enables spark driver and executor to pickup the log4j configuration file from /var/data/ folder mounted to the driver and executor containers. For guide to mount FSx for Lustre to driver and executor containers - refer to EMR Containers integration with FSx for Lustre \"spark.driver.extraJavaOptions\":\"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\", \"spark.executor.extraJavaOptions\":\"-Dlog4j.configuration=file:///var/data/log4j-debug.properties\",","title":"Custom log4j properties"},{"location":"metastore-integrations/docs/","text":"","title":"Index"},{"location":"metastore-integrations/docs/aws-glue/","text":"EMR Containers integration with AWS Glue \u00b6 AWS Glue catalog in same account as EKS \u00b6 In the below example a Spark application will be configured to use AWS Glue data catalog as the hive metastore. gluequery.py cat > gluequery . py << EOF from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Hive integration example\" ) \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"CREATE EXTERNAL TABLE `sparkemrnyc`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/trip-data.parquet/'\" ) spark . sql ( \"SELECT count(*) FROM sparkemrnyc\" ) . show () spark . stop () EOF LOCATION 's3://<s3 prefix>/trip-data.parquet/' Configure the above property to point to the S3 location containing the data. Request cat > Spark - Python - in - s3 - awsglue - log . json << EOF { \"name\" : \"spark-python-in-s3-awsglue-log\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/gluequery.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=3 --conf spark.executor.memory=8G --conf spark.driver.memory=6G --conf spark.executor.cores=3\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.hadoop.hive.metastore.client.factory.class\" : \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" , } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-awsglue-log.json Output from driver logs - Displays the number of rows. +----------+ | count(1)| +----------+ |2716504499| +----------+ AWS Glue catalog in different account \u00b6 The Spark application is submitted to EMR Virtual cluster in Account A and is configured to connect to AWS Glue catalog in Account B. The IAM policy attached to the job execution role (\"executionRoleArn\": \"<execution-role-arn>\") is in Account A { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"glue:*\" ], \"Resource\": [ \"arn:aws:glue:<region>:<account>:catalog\", \"arn:aws:glue:<region>:<account>:database/default\", \"arn:aws:glue:<region>:<account>:table/default/sparkemrnyc\" ] } ] } IAM policy attached to the AWS Glue catalog in Account B { \"Version\" : \"2012-10-17\", \"Statement\" : [ { \"Effect\" : \"Allow\", \"Principal\" : { \"AWS\" : \"<execution-role-arn>\" }, \"Action\" : \"glue:*\", \"Resource\" : [ \"arn:aws:glue:<region>:<account>:catalog\", \"arn:aws:glue:<region>:<account>:database/default\", \"arn:aws:glue:<region>:<account>:table/default/sparkemrnyc\" ] } ] } Request cat > Spark - Python - in - s3 - awsglue - crossaccount . json << EOF { \"name\" : \"spark-python-in-s3-awsglue-crossaccount\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/gluequery.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 \" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.hadoop.hive.metastore.client.factory.class\" : \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" , \"spark.hadoop.hive.metastore.glue.catalogid\" : \"<account B>\" , } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-awsglue-crossaccount.json Configuration of interest To specify the accountID where the AWS Glue catalog is defined reference the following: Spark-Glue integration \"spark.hadoop.hive.metastore.glue.catalogid\":\"<account B>\", Output from driver logs - displays the number of rows. +----------+ | count(1)| +----------+ |2716504499| +----------+","title":"AWS Glue"},{"location":"metastore-integrations/docs/aws-glue/#emr-containers-integration-with-aws-glue","text":"","title":"EMR Containers integration with AWS Glue"},{"location":"metastore-integrations/docs/aws-glue/#aws-glue-catalog-in-same-account-as-eks","text":"In the below example a Spark application will be configured to use AWS Glue data catalog as the hive metastore. gluequery.py cat > gluequery . py << EOF from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Hive integration example\" ) \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"CREATE EXTERNAL TABLE `sparkemrnyc`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/trip-data.parquet/'\" ) spark . sql ( \"SELECT count(*) FROM sparkemrnyc\" ) . show () spark . stop () EOF LOCATION 's3://<s3 prefix>/trip-data.parquet/' Configure the above property to point to the S3 location containing the data. Request cat > Spark - Python - in - s3 - awsglue - log . json << EOF { \"name\" : \"spark-python-in-s3-awsglue-log\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/gluequery.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=3 --conf spark.executor.memory=8G --conf spark.driver.memory=6G --conf spark.executor.cores=3\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.hadoop.hive.metastore.client.factory.class\" : \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" , } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-awsglue-log.json Output from driver logs - Displays the number of rows. +----------+ | count(1)| +----------+ |2716504499| +----------+","title":"AWS Glue catalog in same account as EKS"},{"location":"metastore-integrations/docs/aws-glue/#aws-glue-catalog-in-different-account","text":"The Spark application is submitted to EMR Virtual cluster in Account A and is configured to connect to AWS Glue catalog in Account B. The IAM policy attached to the job execution role (\"executionRoleArn\": \"<execution-role-arn>\") is in Account A { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"glue:*\" ], \"Resource\": [ \"arn:aws:glue:<region>:<account>:catalog\", \"arn:aws:glue:<region>:<account>:database/default\", \"arn:aws:glue:<region>:<account>:table/default/sparkemrnyc\" ] } ] } IAM policy attached to the AWS Glue catalog in Account B { \"Version\" : \"2012-10-17\", \"Statement\" : [ { \"Effect\" : \"Allow\", \"Principal\" : { \"AWS\" : \"<execution-role-arn>\" }, \"Action\" : \"glue:*\", \"Resource\" : [ \"arn:aws:glue:<region>:<account>:catalog\", \"arn:aws:glue:<region>:<account>:database/default\", \"arn:aws:glue:<region>:<account>:table/default/sparkemrnyc\" ] } ] } Request cat > Spark - Python - in - s3 - awsglue - crossaccount . json << EOF { \"name\" : \"spark-python-in-s3-awsglue-crossaccount\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/gluequery.py\" , \"sparkSubmitParameters\" : \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 \" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.hadoop.hive.metastore.client.factory.class\" : \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" , \"spark.hadoop.hive.metastore.glue.catalogid\" : \"<account B>\" , } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-awsglue-crossaccount.json Configuration of interest To specify the accountID where the AWS Glue catalog is defined reference the following: Spark-Glue integration \"spark.hadoop.hive.metastore.glue.catalogid\":\"<account B>\", Output from driver logs - displays the number of rows. +----------+ | count(1)| +----------+ |2716504499| +----------+","title":"AWS Glue catalog in different account"},{"location":"metastore-integrations/docs/hive-metastore/","text":"EMR Containers integration with Hive Metastore \u00b6 Hive metastore Database through JDBC \u00b6 In this example, a Spark application is configured to connect to a Hive Metastore database provisioned with Amazon RDS Aurora MySql. The Amazon RDS and EKS cluster should be in same VPC or else the Spark job will not be able to connect to RDS. Request: cat > Spark - Python - in - s3 - hms - jdbc . json << EOF { \"name\" : \"spark-python-in-s3-hms-jdbc\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/hivejdbc.py\" , \"sparkSubmitParameters\" : \"--jars s3://<s3 prefix>/mariadb-connector-java.jar --conf spark.hadoop.javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver --conf spark.hadoop.javax.jdo.option.ConnectionUserName=<connection-user-name> --conf spark.hadoop.javax.jdo.option.ConnectionPassword=<connection-password> --conf spark.hadoop.javax.jdo.option.ConnectionURL=<JDBC-Connection-string> --conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-hms-jdbc.json In this example we are connecting to mysql db, so mariadb-connector-java.jar needs to be passed with --jars option. If you are using postgres, Oracle or any other database, the appropriate connector jar needs to be included. Configuration of interest: --jars s3://<s3 prefix>/mariadb-connector-java.jar --conf spark.hadoop.javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver --conf spark.hadoop.javax.jdo.option.ConnectionUserName=<connection-user-name> --conf spark.hadoop.javax.jdo.option.ConnectionPassword=<connection-password> --conf spark.hadoop.javax.jdo.option.ConnectionURL**=<JDBC-Connection-string> hivejdbc.py from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"SHOW DATABASES\" ) . show () spark . sql ( \"CREATE EXTERNAL TABLE `ehmsdb`.`sparkemrnyc5`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/nyctaxi_parquet/'\" ) spark . sql ( \"SELECT count(*) FROM ehmsdb.sparkemrnyc5 \" ) . show () spark . stop () The above job lists databases from a remote RDS Hive Metastore, creates a new table and then queries it. Hive metastore thrift service through thrift:// protocol \u00b6 In this example, the spark application is configured to connect to an external Hive metastore thrift server. The thrift server is running on EMR on EC2's master node and AWS RDS Aurora is used as database for the Hive metastore. thriftscript.py: hive.metastore.uris config needs to be set to read from external Hive metastore. from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . config ( \"hive.metastore.uris\" , \"<hive metastore thrift uri>\" ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"SHOW DATABASES\" ) . show () spark . sql ( \"CREATE EXTERNAL TABLE ehmsdb.`sparkemrnyc2`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/nyctaxi_parquet/'\" ) spark . sql ( \"SELECT * FROM ehmsdb.sparkemrnyc2\" ) . show () spark . stop () Request: The below job lists databases from remote Hive Metastore, creates a new table and then queries it. cat > Spark - Python - in - s3 - hms - thrift . json << EOF { \"name\" : \"spark-python-in-s3-hms-thrift\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/thriftscript.py\" , \"sparkSubmitParameters\" : \"--jars s3://<s3 prefix>/mariadb-connector-java.jar --conf spark.hadoop.javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver --conf spark.hadoop.javax.jdo.option.ConnectionUserName=<connection-user-name> --conf spark.hadoop.javax.jdo.option.ConnectionPassword=<connection-password> --conf spark.hadoop.javax.jdo.option.ConnectionURL=<JDBC-Connection-string> --conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-hms-thrift.json","title":"Hive Metastore"},{"location":"metastore-integrations/docs/hive-metastore/#emr-containers-integration-with-hive-metastore","text":"","title":"EMR Containers integration with Hive Metastore"},{"location":"metastore-integrations/docs/hive-metastore/#hive-metastore-database-through-jdbc","text":"In this example, a Spark application is configured to connect to a Hive Metastore database provisioned with Amazon RDS Aurora MySql. The Amazon RDS and EKS cluster should be in same VPC or else the Spark job will not be able to connect to RDS. Request: cat > Spark - Python - in - s3 - hms - jdbc . json << EOF { \"name\" : \"spark-python-in-s3-hms-jdbc\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/hivejdbc.py\" , \"sparkSubmitParameters\" : \"--jars s3://<s3 prefix>/mariadb-connector-java.jar --conf spark.hadoop.javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver --conf spark.hadoop.javax.jdo.option.ConnectionUserName=<connection-user-name> --conf spark.hadoop.javax.jdo.option.ConnectionPassword=<connection-password> --conf spark.hadoop.javax.jdo.option.ConnectionURL=<JDBC-Connection-string> --conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-hms-jdbc.json In this example we are connecting to mysql db, so mariadb-connector-java.jar needs to be passed with --jars option. If you are using postgres, Oracle or any other database, the appropriate connector jar needs to be included. Configuration of interest: --jars s3://<s3 prefix>/mariadb-connector-java.jar --conf spark.hadoop.javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver --conf spark.hadoop.javax.jdo.option.ConnectionUserName=<connection-user-name> --conf spark.hadoop.javax.jdo.option.ConnectionPassword=<connection-password> --conf spark.hadoop.javax.jdo.option.ConnectionURL**=<JDBC-Connection-string> hivejdbc.py from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"SHOW DATABASES\" ) . show () spark . sql ( \"CREATE EXTERNAL TABLE `ehmsdb`.`sparkemrnyc5`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/nyctaxi_parquet/'\" ) spark . sql ( \"SELECT count(*) FROM ehmsdb.sparkemrnyc5 \" ) . show () spark . stop () The above job lists databases from a remote RDS Hive Metastore, creates a new table and then queries it.","title":"Hive metastore Database through JDBC"},{"location":"metastore-integrations/docs/hive-metastore/#hive-metastore-thrift-service-through-thrift-protocol","text":"In this example, the spark application is configured to connect to an external Hive metastore thrift server. The thrift server is running on EMR on EC2's master node and AWS RDS Aurora is used as database for the Hive metastore. thriftscript.py: hive.metastore.uris config needs to be set to read from external Hive metastore. from os.path import expanduser , join , abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . config ( \"hive.metastore.uris\" , \"<hive metastore thrift uri>\" ) \\ . enableHiveSupport () \\ . getOrCreate () spark . sql ( \"SHOW DATABASES\" ) . show () spark . sql ( \"CREATE EXTERNAL TABLE ehmsdb.`sparkemrnyc2`( `dispatching_base_num` string, `pickup_datetime` string, `dropoff_datetime` string, `pulocationid` bigint, `dolocationid` bigint, `sr_flag` bigint) STORED AS PARQUET LOCATION 's3://<s3 prefix>/nyctaxi_parquet/'\" ) spark . sql ( \"SELECT * FROM ehmsdb.sparkemrnyc2\" ) . show () spark . stop () Request: The below job lists databases from remote Hive Metastore, creates a new table and then queries it. cat > Spark - Python - in - s3 - hms - thrift . json << EOF { \"name\" : \"spark-python-in-s3-hms-thrift\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/thriftscript.py\" , \"sparkSubmitParameters\" : \"--jars s3://<s3 prefix>/mariadb-connector-java.jar --conf spark.hadoop.javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver --conf spark.hadoop.javax.jdo.option.ConnectionUserName=<connection-user-name> --conf spark.hadoop.javax.jdo.option.ConnectionPassword=<connection-password> --conf spark.hadoop.javax.jdo.option.ConnectionURL=<JDBC-Connection-string> --conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3-hms-thrift.json","title":"Hive metastore thrift service through thrift:// protocol"},{"location":"node-placement/docs/","text":"","title":"Index"},{"location":"node-placement/docs/eks-node-placement/","text":"EKS Node Placement \u00b6 Single AZ placement \u00b6 AWS EKS clusters can span multiple AZs in a VPC. A Spark application whose driver and executor pods are distributed across multiple AZs can incur inter-AZ data transfer costs. To minimize or eliminate inter-AZ data transfer costs, you can configure the application to only run on the nodes within a single AZ. In this example, we use the kubernetes node selector to specify which AZ should the job run on. Request: cat >spark-python-in-s3-nodeselector.json << EOF { \"name\": \"spark-python-in-s3-nodeselector\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='<availability zone>' --conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-nodeselector.json Observed Behavior: When the job starts the driver pod and executor pods are scheduled only on those EKS worker nodes with the label topology.kubernetes.io/zone: <availability zone> . This ensures the spark job is run within a single AZ. If there are not enough resources within the specified AZ, the pods will be in the pending state until the Autoscaler(if configured) kicks in or more resources become available. Spark on kubernetes Node selector configuration Kubernetes Node selector reference Configuration of interest - --conf spark.kubernetes.node.selector.zone='<availability zone>' zone is a built in label that EKS assigns to every EKS worker Node. The above config will ensure to schedule the driver and executor pod on those EKS worker nodes labeled - topology.kubernetes.io/zone: <availability zone> . However, user defined labels can also be assigned to EKS worker nodes and used as node selector. Other common use cases are using node labels to force the job to run on on demand/spot, machine type, etc. Single AZ and ec2 instance type placement \u00b6 Multiple key value pairs for spark.kubernetes.node.selector.[labelKey] can be passed to add filter conditions for selecting the EKS worker node. If you want to schedule on EKS worker nodes in <availability zone> and instance-type as m5.4xlarge - it is done as below Request: { \"name\": \"spark-python-in-s3-nodeselector\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.kubernetes.pyspark.pythonVersion=3 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 --conf spark.sql.shuffle.partitions=1000\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"false\", \"spark.kubernetes.node.selector.topology.kubernetes.io/zone\":\"<availability zone>\", \"spark.kubernetes.node.selector.node.kubernetes.io/instance-type\":\"m5.4xlarge\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } } } Configuration of interest spark.kubernetes.node.selector.[labelKey] - Adds to the node selector of the driver pod and executor pods, with key labelKey and the value as the configuration's value. For example, setting spark.kubernetes.node.selector.identifier to myIdentifier will result in the driver pod and executors having a node selector with key identifier and value myIdentifier. Multiple node selector keys can be added by setting multiple configurations with this prefix.","title":"EKS Node placement"},{"location":"node-placement/docs/eks-node-placement/#eks-node-placement","text":"","title":"EKS Node Placement"},{"location":"node-placement/docs/eks-node-placement/#single-az-placement","text":"AWS EKS clusters can span multiple AZs in a VPC. A Spark application whose driver and executor pods are distributed across multiple AZs can incur inter-AZ data transfer costs. To minimize or eliminate inter-AZ data transfer costs, you can configure the application to only run on the nodes within a single AZ. In this example, we use the kubernetes node selector to specify which AZ should the job run on. Request: cat >spark-python-in-s3-nodeselector.json << EOF { \"name\": \"spark-python-in-s3-nodeselector\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='<availability zone>' --conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-nodeselector.json Observed Behavior: When the job starts the driver pod and executor pods are scheduled only on those EKS worker nodes with the label topology.kubernetes.io/zone: <availability zone> . This ensures the spark job is run within a single AZ. If there are not enough resources within the specified AZ, the pods will be in the pending state until the Autoscaler(if configured) kicks in or more resources become available. Spark on kubernetes Node selector configuration Kubernetes Node selector reference Configuration of interest - --conf spark.kubernetes.node.selector.zone='<availability zone>' zone is a built in label that EKS assigns to every EKS worker Node. The above config will ensure to schedule the driver and executor pod on those EKS worker nodes labeled - topology.kubernetes.io/zone: <availability zone> . However, user defined labels can also be assigned to EKS worker nodes and used as node selector. Other common use cases are using node labels to force the job to run on on demand/spot, machine type, etc.","title":"Single AZ placement"},{"location":"node-placement/docs/eks-node-placement/#single-az-and-ec2-instance-type-placement","text":"Multiple key value pairs for spark.kubernetes.node.selector.[labelKey] can be passed to add filter conditions for selecting the EKS worker node. If you want to schedule on EKS worker nodes in <availability zone> and instance-type as m5.4xlarge - it is done as below Request: { \"name\": \"spark-python-in-s3-nodeselector\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.kubernetes.pyspark.pythonVersion=3 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 --conf spark.sql.shuffle.partitions=1000\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"false\", \"spark.kubernetes.node.selector.topology.kubernetes.io/zone\":\"<availability zone>\", \"spark.kubernetes.node.selector.node.kubernetes.io/instance-type\":\"m5.4xlarge\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } } } Configuration of interest spark.kubernetes.node.selector.[labelKey] - Adds to the node selector of the driver pod and executor pods, with key labelKey and the value as the configuration's value. For example, setting spark.kubernetes.node.selector.identifier to myIdentifier will result in the driver pod and executors having a node selector with key identifier and value myIdentifier. Multiple node selector keys can be added by setting multiple configurations with this prefix.","title":"Single AZ and ec2 instance type placement"},{"location":"node-placement/docs/fargate-node-placement/","text":"","title":"Fargate node placement"},{"location":"outposts/","text":"","title":"Index"},{"location":"outposts/emr-containers-on-outposts/","text":"Running EMR Containers on AWS Outposts \u00b6 Background \u00b6 You can now create and run Amazon EMR Container jobs on AWS EKS clusters running on AWS Outposts. AWS Outposts enables native AWS services, infrastructure, and operating models in on-premises facilities. In AWS Outposts environments, you can use the same AWS APIs, tools, and infrastructure that you use in the AWS Cloud. Amazon EKS nodes on AWS Outposts is ideal for low-latency workloads that need to be run in close proximity to on-premises data and applications. Visit the AWS EKS on Outposts documentation page for more information, pre-requisites and considerations. This document provides the steps necessary to setup EMR Containers on AWS Outposts. Key Considerations and Recommendations \u00b6 The EKS Cluster being created on the Outpost must be created with self-managed node groups. Use the AWS Management Console+CloudFormation to create a self-managed node group in Outposts For EMR workloads, we recommend creating EKS clusters where all the worker nodes reside in the Outposts self-managed node group. AWS Fargate is not available on Outposts. Regions supported, prerequisites and considerations for EKS on Outposts - https://docs.aws.amazon.com/eks/latest/userguide/eks-on-outposts.html Infrastructure Setup \u00b6 Setup EKS on Outposts \u00b6 Network Setup Setup a VPC aws ec2 create-vpc \\ --region <us-west-2> \\ --cidr-block '<10.0.0.0/16>' In the output that's returned, take note of the VPC ID. { \"Vpc\": { \"VpcId\": \"vpc-123vpc\", ... } } Create two subnets in the parent region aws ec2 create - subnet \\ --region '<us-west-2>' \\ --availability-zone-id '<usw2-az1>' \\ --vpc-id '<vpc-123vpc>' \\ --cidr-block '<10.0.1.0/24>' aws ec2 create - subnet \\ --region '<us-west-2>' \\ --availability-zone-id '<usw2-az2>' \\ --vpc-id '<vpc-123vpc>' \\ --cidr-block '<10.0.2.0/24>' In the output that's returned, take note of the Subnet ID. { \"Subnet\": { \"SubnetId\": \"subnet-111\", ... } } { \"Subnet\": { \"SubnetId\": \"subnet-222\", ... } } Create a subnet in the Outpost AZ (This step is different for Outposts) aws ec2 create-subnet \\ --region '<us-west-2>' \\ --availability-zone-id '<usw2-az1>' \\ --outpost-arn 'arn:aws:outposts:<us-west-2>:<123456789>:outpost/<op-123op>' \\ --vpc-id '<vpc-123vpc>' \\ --cidr-block '<10.0.3.0/24>' In the output that's returned, take note of the Subnet ID. { \"Subnet\": { \"SubnetId\": \"subnet-333outpost\", \"OutpostArn\": \"...\" ... } } EKS Cluster Creation Create an EKS cluster using the three subnet Ids created earlier aws eks create-cluster \\ --region '<us-west-2>' \\ --name '<outposts-eks-cluster>' \\ --role-arn 'arn:aws:iam::<123456789>:role/<cluster-service-role>' \\ --resources-vpc-config subnetIds='<subnet-111>,<subnet-222>,<subnet-333outpost>' Check until the cluster status becomes active aws eks describe-cluster \\ --region '<us-west-2>' \\ --name '<outposts-eks-cluster>' Note the values of resourcesVpcConfig.clusterSecurityGroupId and identity.oidc.issuer { \"cluster\": { \"name\": \"outposts-eks-cluster\", ... \"resourcesVpcConfig\": { \"clusterSecurityGroupId\": \"sg-123clustersg\", }, \"identity\": { \"oidc\": { \"issuer\": \"https://oidc.eks.us-west-2.amazonaws.com/id/oidcid\" } }, \"status\": \"ACTIVE\", } } Add the Outposts nodes to the EKS Cluster. At this point, eksctl cannot be used to launch self-managed node groups in Outposts. Please follow the steps listed in the self-managed nodes documentation page . In order to use the cloudformation script lised in the AWS Management Console tab, make note of the following values created in the earlier steps: ClusterName: <outposts-eks-cluster> ClusterControlPlaneSecurityGroup: <sg-123clustersg> Subnets: <subnet-333outpost> Apply the aws-auth-cm config map listed on the documentation page to allow the nodes to join the cluster. Register cluster with EMR Containers \u00b6 Once the EKS cluster has been created and the nodes registered with EKS control plane, the following steps need to be performed: Enable cluster access for Amazon EMR on EKS Enable IAM Roles for Service Accounts (IRSA) on the EKS cluster Create a job execution role Update the trust policy of the job execution role Grant users access to Amazon EMR on EKS Register the Amazon EKS cluster with Amazon EMR Conclusion \u00b6 EMR-EKS on Outposts allows users to run their big data jobs in close proximity to on-premises data and applications.","title":"EMR on EKS(AWS Outposts)"},{"location":"outposts/emr-containers-on-outposts/#running-emr-containers-on-aws-outposts","text":"","title":"Running EMR Containers on AWS Outposts"},{"location":"outposts/emr-containers-on-outposts/#background","text":"You can now create and run Amazon EMR Container jobs on AWS EKS clusters running on AWS Outposts. AWS Outposts enables native AWS services, infrastructure, and operating models in on-premises facilities. In AWS Outposts environments, you can use the same AWS APIs, tools, and infrastructure that you use in the AWS Cloud. Amazon EKS nodes on AWS Outposts is ideal for low-latency workloads that need to be run in close proximity to on-premises data and applications. Visit the AWS EKS on Outposts documentation page for more information, pre-requisites and considerations. This document provides the steps necessary to setup EMR Containers on AWS Outposts.","title":"Background"},{"location":"outposts/emr-containers-on-outposts/#key-considerations-and-recommendations","text":"The EKS Cluster being created on the Outpost must be created with self-managed node groups. Use the AWS Management Console+CloudFormation to create a self-managed node group in Outposts For EMR workloads, we recommend creating EKS clusters where all the worker nodes reside in the Outposts self-managed node group. AWS Fargate is not available on Outposts. Regions supported, prerequisites and considerations for EKS on Outposts - https://docs.aws.amazon.com/eks/latest/userguide/eks-on-outposts.html","title":"Key Considerations and Recommendations"},{"location":"outposts/emr-containers-on-outposts/#infrastructure-setup","text":"","title":"Infrastructure Setup"},{"location":"outposts/emr-containers-on-outposts/#setup-eks-on-outposts","text":"Network Setup Setup a VPC aws ec2 create-vpc \\ --region <us-west-2> \\ --cidr-block '<10.0.0.0/16>' In the output that's returned, take note of the VPC ID. { \"Vpc\": { \"VpcId\": \"vpc-123vpc\", ... } } Create two subnets in the parent region aws ec2 create - subnet \\ --region '<us-west-2>' \\ --availability-zone-id '<usw2-az1>' \\ --vpc-id '<vpc-123vpc>' \\ --cidr-block '<10.0.1.0/24>' aws ec2 create - subnet \\ --region '<us-west-2>' \\ --availability-zone-id '<usw2-az2>' \\ --vpc-id '<vpc-123vpc>' \\ --cidr-block '<10.0.2.0/24>' In the output that's returned, take note of the Subnet ID. { \"Subnet\": { \"SubnetId\": \"subnet-111\", ... } } { \"Subnet\": { \"SubnetId\": \"subnet-222\", ... } } Create a subnet in the Outpost AZ (This step is different for Outposts) aws ec2 create-subnet \\ --region '<us-west-2>' \\ --availability-zone-id '<usw2-az1>' \\ --outpost-arn 'arn:aws:outposts:<us-west-2>:<123456789>:outpost/<op-123op>' \\ --vpc-id '<vpc-123vpc>' \\ --cidr-block '<10.0.3.0/24>' In the output that's returned, take note of the Subnet ID. { \"Subnet\": { \"SubnetId\": \"subnet-333outpost\", \"OutpostArn\": \"...\" ... } } EKS Cluster Creation Create an EKS cluster using the three subnet Ids created earlier aws eks create-cluster \\ --region '<us-west-2>' \\ --name '<outposts-eks-cluster>' \\ --role-arn 'arn:aws:iam::<123456789>:role/<cluster-service-role>' \\ --resources-vpc-config subnetIds='<subnet-111>,<subnet-222>,<subnet-333outpost>' Check until the cluster status becomes active aws eks describe-cluster \\ --region '<us-west-2>' \\ --name '<outposts-eks-cluster>' Note the values of resourcesVpcConfig.clusterSecurityGroupId and identity.oidc.issuer { \"cluster\": { \"name\": \"outposts-eks-cluster\", ... \"resourcesVpcConfig\": { \"clusterSecurityGroupId\": \"sg-123clustersg\", }, \"identity\": { \"oidc\": { \"issuer\": \"https://oidc.eks.us-west-2.amazonaws.com/id/oidcid\" } }, \"status\": \"ACTIVE\", } } Add the Outposts nodes to the EKS Cluster. At this point, eksctl cannot be used to launch self-managed node groups in Outposts. Please follow the steps listed in the self-managed nodes documentation page . In order to use the cloudformation script lised in the AWS Management Console tab, make note of the following values created in the earlier steps: ClusterName: <outposts-eks-cluster> ClusterControlPlaneSecurityGroup: <sg-123clustersg> Subnets: <subnet-333outpost> Apply the aws-auth-cm config map listed on the documentation page to allow the nodes to join the cluster.","title":"Setup EKS on Outposts"},{"location":"outposts/emr-containers-on-outposts/#register-cluster-with-emr-containers","text":"Once the EKS cluster has been created and the nodes registered with EKS control plane, the following steps need to be performed: Enable cluster access for Amazon EMR on EKS Enable IAM Roles for Service Accounts (IRSA) on the EKS cluster Create a job execution role Update the trust policy of the job execution role Grant users access to Amazon EMR on EKS Register the Amazon EKS cluster with Amazon EMR","title":"Register cluster with EMR Containers"},{"location":"outposts/emr-containers-on-outposts/#conclusion","text":"EMR-EKS on Outposts allows users to run their big data jobs in close proximity to on-premises data and applications.","title":"Conclusion"},{"location":"performance/docs/","text":"","title":"Index"},{"location":"performance/docs/dra/","text":"Dynamic Resource Allocation \u00b6 DRA is available in Spark 3 (EMR 6.x) without the need for an external shuffle service. Spark on Kubernetes doesn't support external shuffle service as of spark 3.1, but DRA can be achieved by enabling shuffle tracking . Spark DRA without external shuffle service: With DRA, the spark driver spawns the initial number of executors and then scales up the number until the specified maximum number of executors is met to process the pending tasks. Idle executors are terminated when there are no pending tasks, the executor idle time exceeds the idle timeout( spark.dynamicAllocation.executorIdleTimeout) and it doesn't have any cached or shuffle data. If the executor idle threshold is reached and it has cached data, then it has to exceed the cache data idle timeout( spark.dynamicAllocation.cachedExecutorIdleTimeout) and if the executor doesn't have shuffle data, then the idle executor is terminated. If the executor idle threshold is reached and it has shuffle data, then without external shuffle service the executor will never be terminated. These executors will be terminated when the job is completed. This behavior is enforced by \"spark.dynamicAllocation.shuffleTracking.enabled\":\"true\" and \"spark.dynamicAllocation.enabled\":\"true\" If \"spark.dynamicAllocation.shuffleTracking.enabled\":\"false\"and \"spark.dynamicAllocation.enabled\":\"true\" then the spark application will error out since external shuffle service is not available. Request: cat >spark-python-in-s3-dra.json << EOF { \"name\": \"spark-python-in-s3-dra\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"true\", \"spark.dynamicAllocation.shuffleTracking.enabled\":\"true\", \"spark.dynamicAllocation.minExecutors\":\"5\", \"spark.dynamicAllocation.maxExecutors\":\"100\", \"spark.dynamicAllocation.initialExecutors\":\"10\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-dra.json Observed Behavior: When the job gets started, the driver pod gets created and 10 executors are initially created. ( \"spark.dynamicAllocation.initialExecutors\":\"10\" ) Then the number of executors can scale up to a maximum of 100 ( \"spark.dynamicAllocation.maxExecutors\":\"100\" ). Configurations to note: Please note that this feature is marked as Experimental as of Spark 3.0.0 spark.dynamicAllocation.shuffleTracking.enabled - ** Experimental ** . Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service. This option will try to keep alive executors that are storing shuffle data for active jobs. spark.dynamicAllocation.shuffleTracking.timeout - When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle data. The default value means that Spark will rely on the shuffles being garbage collected to be able to release executors. If for some reason garbage collection is not cleaning up shuffles quickly enough, this option can be used to control when to time out executors even when they are storing shuffle data.","title":"Dynamic Resource Allocation"},{"location":"performance/docs/dra/#dynamic-resource-allocation","text":"DRA is available in Spark 3 (EMR 6.x) without the need for an external shuffle service. Spark on Kubernetes doesn't support external shuffle service as of spark 3.1, but DRA can be achieved by enabling shuffle tracking . Spark DRA without external shuffle service: With DRA, the spark driver spawns the initial number of executors and then scales up the number until the specified maximum number of executors is met to process the pending tasks. Idle executors are terminated when there are no pending tasks, the executor idle time exceeds the idle timeout( spark.dynamicAllocation.executorIdleTimeout) and it doesn't have any cached or shuffle data. If the executor idle threshold is reached and it has cached data, then it has to exceed the cache data idle timeout( spark.dynamicAllocation.cachedExecutorIdleTimeout) and if the executor doesn't have shuffle data, then the idle executor is terminated. If the executor idle threshold is reached and it has shuffle data, then without external shuffle service the executor will never be terminated. These executors will be terminated when the job is completed. This behavior is enforced by \"spark.dynamicAllocation.shuffleTracking.enabled\":\"true\" and \"spark.dynamicAllocation.enabled\":\"true\" If \"spark.dynamicAllocation.shuffleTracking.enabled\":\"false\"and \"spark.dynamicAllocation.enabled\":\"true\" then the spark application will error out since external shuffle service is not available. Request: cat >spark-python-in-s3-dra.json << EOF { \"name\": \"spark-python-in-s3-dra\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\":\"true\", \"spark.dynamicAllocation.shuffleTracking.enabled\":\"true\", \"spark.dynamicAllocation.minExecutors\":\"5\", \"spark.dynamicAllocation.maxExecutors\":\"100\", \"spark.dynamicAllocation.initialExecutors\":\"10\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-dra.json Observed Behavior: When the job gets started, the driver pod gets created and 10 executors are initially created. ( \"spark.dynamicAllocation.initialExecutors\":\"10\" ) Then the number of executors can scale up to a maximum of 100 ( \"spark.dynamicAllocation.maxExecutors\":\"100\" ). Configurations to note: Please note that this feature is marked as Experimental as of Spark 3.0.0 spark.dynamicAllocation.shuffleTracking.enabled - ** Experimental ** . Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service. This option will try to keep alive executors that are storing shuffle data for active jobs. spark.dynamicAllocation.shuffleTracking.timeout - When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle data. The default value means that Spark will rely on the shuffles being garbage collected to be able to release executors. If for some reason garbage collection is not cleaning up shuffles quickly enough, this option can be used to control when to time out executors even when they are storing shuffle data.","title":"Dynamic Resource Allocation"},{"location":"security/docs/","text":"","title":"Index"},{"location":"security/docs/spark/data-encryption/","text":"EMR Containers Spark - In transit and At Rest data encryption \u00b6 Encryption at Rest \u00b6 Amazon S3 Client-Side Encryption \u00b6 To utilize S3 Client side encryption , you will need to create a KMS Key to be used to encrypt and decrypt data. If you do not have an KMS key, please follow this guide - AWS KMS create keys . Also please note the job execution role needs access to this key, please see Add to Key policy for instructions on how to add these permissions. trip-count-encrypt-write.py: cat > trip - count - encrypt - write . py << EOF import sys from pyspark.sql import SparkSession if __name__ == \"__main__\" : spark = SparkSession \\ . builder \\ . appName ( \"trip-count-join-fsx\" ) \\ . getOrCreate () df = spark . read . parquet ( 's3://<s3 prefix>/trip-data.parquet' ) print ( \"Total trips: \" + str ( df . count ())) df . write . parquet ( 's3://<s3 prefix>/write-encrypt-trip-data.parquet' ) print ( \"Encrypt - KMS- CSE writew to s3 compeleted\" ) spark . stop () EOF Request: cat > spark - python - in - s3 - encrypt - cse - kms - write . json << EOF { \"name\" : \"spark-python-in-s3-encrypt-cse-kms-write\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>trip-count-encrypt-write.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=10 --conf spark.driver.cores=2 --conf spark.executor.memory=20G --conf spark.driver.memory=20G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } , { \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.cse.enabled\" : \"true\" , \"fs.s3.cse.encryptionMaterialsProvider\" : \"com.amazon.ws.emr.hadoop.fs.cse.KMSEncryptionMaterialsProvider\" , \"fs.s3.cse.kms.keyId\" : \"<KMS Key Id>\" } } ], \"monitoringConfiguration\" : { \"persistentAppUI\" : \"ENABLED\" , \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-encrypt-cse-kms-write.json In the above request, EMRFS encrypts the parquet file with the specified KMS key and the encrypted object is persisted to the specified s3 location. To verify the encryption - use the same KMS key to decrypt - the KMS key used is a symmetric key ( the same key can be used to both encrypt and decrypt) trip-count-encrypt-read.py cat > trip - count - encrypt - read . py << EOF import sys from pyspark.sql import SparkSession if __name__ == \"__main__\" : spark = SparkSession \\ . builder \\ . appName ( \"trip-count-join-fsx\" ) \\ . getOrCreate () df = spark . read . parquet ( 's3://<s3 prefix>/trip-data.parquet' ) print ( \"Total trips: \" + str ( df . count ())) df_encrypt = spark . read . parquet ( 's3://<s3 prefix>/write-encrypt-trip-data.parquet' ) print ( \"Encrypt data - Total trips: \" + str ( df_encrypt . count ())) spark . stop () EOF Request cat > spark - python - in - s3 - encrypt - cse - kms - read . json << EOF { \"name\" : \"spark-python-in-s3-encrypt-cse-kms-read\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>trip-count-encrypt-write.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=10 --conf spark.driver.cores=2 --conf spark.executor.memory=20G --conf spark.driver.memory=20G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } , { \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.cse.enabled\" : \"true\" , \"fs.s3.cse.encryptionMaterialsProvider\" : \"com.amazon.ws.emr.hadoop.fs.cse.KMSEncryptionMaterialsProvider\" , \"fs.s3.cse.kms.keyId\" : \"<KMS Key Id>\" } } ], \"monitoringConfiguration\" : { \"persistentAppUI\" : \"ENABLED\" , \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-encrypt-cse-kms-read.json Validate encryption: Try to read the encrypted data without specifying \"fs.s3.cse.enabled\":\"true\" - will get an error message in the driver and executor logs because the content is encrypted and cannot be read without decryption.","title":"Data Encryption"},{"location":"security/docs/spark/data-encryption/#emr-containers-spark-in-transit-and-at-rest-data-encryption","text":"","title":"EMR Containers Spark - In transit and At Rest data encryption"},{"location":"security/docs/spark/data-encryption/#encryption-at-rest","text":"","title":"Encryption at Rest"},{"location":"security/docs/spark/data-encryption/#amazon-s3-client-side-encryption","text":"To utilize S3 Client side encryption , you will need to create a KMS Key to be used to encrypt and decrypt data. If you do not have an KMS key, please follow this guide - AWS KMS create keys . Also please note the job execution role needs access to this key, please see Add to Key policy for instructions on how to add these permissions. trip-count-encrypt-write.py: cat > trip - count - encrypt - write . py << EOF import sys from pyspark.sql import SparkSession if __name__ == \"__main__\" : spark = SparkSession \\ . builder \\ . appName ( \"trip-count-join-fsx\" ) \\ . getOrCreate () df = spark . read . parquet ( 's3://<s3 prefix>/trip-data.parquet' ) print ( \"Total trips: \" + str ( df . count ())) df . write . parquet ( 's3://<s3 prefix>/write-encrypt-trip-data.parquet' ) print ( \"Encrypt - KMS- CSE writew to s3 compeleted\" ) spark . stop () EOF Request: cat > spark - python - in - s3 - encrypt - cse - kms - write . json << EOF { \"name\" : \"spark-python-in-s3-encrypt-cse-kms-write\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>trip-count-encrypt-write.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=10 --conf spark.driver.cores=2 --conf spark.executor.memory=20G --conf spark.driver.memory=20G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } , { \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.cse.enabled\" : \"true\" , \"fs.s3.cse.encryptionMaterialsProvider\" : \"com.amazon.ws.emr.hadoop.fs.cse.KMSEncryptionMaterialsProvider\" , \"fs.s3.cse.kms.keyId\" : \"<KMS Key Id>\" } } ], \"monitoringConfiguration\" : { \"persistentAppUI\" : \"ENABLED\" , \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-encrypt-cse-kms-write.json In the above request, EMRFS encrypts the parquet file with the specified KMS key and the encrypted object is persisted to the specified s3 location. To verify the encryption - use the same KMS key to decrypt - the KMS key used is a symmetric key ( the same key can be used to both encrypt and decrypt) trip-count-encrypt-read.py cat > trip - count - encrypt - read . py << EOF import sys from pyspark.sql import SparkSession if __name__ == \"__main__\" : spark = SparkSession \\ . builder \\ . appName ( \"trip-count-join-fsx\" ) \\ . getOrCreate () df = spark . read . parquet ( 's3://<s3 prefix>/trip-data.parquet' ) print ( \"Total trips: \" + str ( df . count ())) df_encrypt = spark . read . parquet ( 's3://<s3 prefix>/write-encrypt-trip-data.parquet' ) print ( \"Encrypt data - Total trips: \" + str ( df_encrypt . count ())) spark . stop () EOF Request cat > spark - python - in - s3 - encrypt - cse - kms - read . json << EOF { \"name\" : \"spark-python-in-s3-encrypt-cse-kms-read\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>trip-count-encrypt-write.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=10 --conf spark.driver.cores=2 --conf spark.executor.memory=20G --conf spark.driver.memory=20G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } , { \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.cse.enabled\" : \"true\" , \"fs.s3.cse.encryptionMaterialsProvider\" : \"com.amazon.ws.emr.hadoop.fs.cse.KMSEncryptionMaterialsProvider\" , \"fs.s3.cse.kms.keyId\" : \"<KMS Key Id>\" } } ], \"monitoringConfiguration\" : { \"persistentAppUI\" : \"ENABLED\" , \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-encrypt-cse-kms-read.json Validate encryption: Try to read the encrypted data without specifying \"fs.s3.cse.enabled\":\"true\" - will get an error message in the driver and executor logs because the content is encrypted and cannot be read without decryption.","title":"Amazon S3 Client-Side Encryption"},{"location":"security/docs/spark/network-security/","text":"","title":"Network security"},{"location":"storage/docs/","text":"","title":"Index"},{"location":"storage/docs/spark/ebs/","text":"Mount EBS Volume to spark driver and executor pods \u00b6 Amazon EBS volumes can be mounted on Spark driver and executor pods through static and dynamic provisioning. EKS support for EBS CSI driver Documentation for EBS CSI driver Static Provisioning \u00b6 Static Provisioning \u00b6 EKS Admin Tasks \u00b6 First, create your EBS volumes: aws ec2 --region <region> create-volume --availability-zone <availability zone> --size 50 { \"AvailabilityZone\": \"<availability zone>\", \"MultiAttachEnabled\": false, \"Tags\": [], \"Encrypted\": false, \"VolumeType\": \"gp2\", \"VolumeId\": \"<vol -id>\", \"State\": \"creating\", \"Iops\": 150, \"SnapshotId\": \"\", \"CreateTime\": \"2020-11-03T18:36:21.000Z\", \"Size\": 50 } Create Persistent Volume(PV) that has the EBS volume created above hardcoded: cat > ebs - static - pv . yaml << EOF apiVersion : v1 kind : PersistentVolume metadata : name : ebs - static - pv spec : capacity : storage : 5 Gi accessModes : - ReadWriteOnce storageClassName : gp2 awsElasticBlockStore : fsType : ext4 volumeID : < vol - id > EOF kubectl apply - f ebs - static - pv . yaml - n < namespace > Create Persistent Volume Claim(PVC) for the Persistent Volume created above: cat > ebs - static - pvc . yaml << EOF kind : PersistentVolumeClaim apiVersion : v1 metadata : name : ebs - static - pvc spec : accessModes : - ReadWriteOnce resources : requests : storage : 5 Gi volumeName : ebs - static - pv EOF kubectl apply - f ebs - static - pvc . yaml - n < namespace > PVC - ebs-static-pvc can be used by spark developer to mount to the spark pod NOTE : Pods running in EKS worker nodes can only attach to the EBS volume provisioned in the same AZ as the EKS worker node. Use node selectors to schedule pods on EKS worker nodes the specified AZ. Spark Developer Tasks \u00b6 Request cat >spark-python-in-s3-ebs-static-localdir.json << EOF { \"name\": \"spark-python-in-s3-ebs-static-localdir\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.instances=10 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 \" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.options.claimName\":\"ebs-static-pvc\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.readOnly\":\"false\", } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-ebs-static-localdir.json Observed Behavior: When the job gets started, the pre-provisioned EBS volume is mounted to driver pod. You can exec into the driver container to verify that the EBS volume is mounted. Also you can verify the mount from the driver pod's spec. kubectl get pod <driver pod name> -n <namespace> -o yaml --export Dynamic Provisioning \u00b6 EKS Admin Tasks \u00b6 Create EBS Storage Class cat > demo - gp2 - sc . yaml << EOF apiVersion : storage . k8s . io / v1 kind : StorageClass metadata : name : demo - gp2 - sc provisioner : kubernetes . io / aws - ebs parameters : type : gp2 reclaimPolicy : Retain allowVolumeExpansion : true mountOptions : - debug volumeBindingMode : Immediate EOF kubectl apply - f demo - gp2 - sc . yaml create Persistent Volume for the EBS storage class - demo-gp2-sc cat > ebs - demo - gp2 - claim . yaml << EOF apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ebs - demo - gp2 - claim labels : app : chicago spec : storageClassName : demo - gp2 - sc accessModes : - ReadWriteOnce resources : requests : storage : 100 Gi EOF kubectl apply - f ebs - demo - gp2 - claim . yaml - n < namespace > Spark Developer Tasks \u00b6 Request cat >spark-python-in-s3-ebs-dynamic-localdir.json << EOF { \"name\": \"spark-python-in-s3-ebs-dynamic-localdir\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.instances=10 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.options.claimName\":\"ebs-demo-gp2-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.readOnly\":\"false\", } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-ebs-dynamic-localdir.json Observed Behavior: When the job gets started an EBS volume is provisioned dynamically by the EBS CSI driver and mounted to the driver pod. You can exec into the driver container to verify that the EBS volume is mounted. Also, you can verify the mount from driver pod spec. kubectl get pod <driver pod name> -n <namespace> -o yaml --export POINT TO NOTE : It is not possible to use this dynamic provisioning strategy for EBS to spark executors. It is not possible to mount a new EBS volume to every Spark executor. Instead use a distributed file system like Lustre, EFS, NFS to mount to executors.","title":"EBS"},{"location":"storage/docs/spark/ebs/#mount-ebs-volume-to-spark-driver-and-executor-pods","text":"Amazon EBS volumes can be mounted on Spark driver and executor pods through static and dynamic provisioning. EKS support for EBS CSI driver Documentation for EBS CSI driver","title":"Mount EBS Volume to spark driver and executor pods"},{"location":"storage/docs/spark/ebs/#static-provisioning","text":"","title":"Static Provisioning"},{"location":"storage/docs/spark/ebs/#static-provisioning_1","text":"","title":"Static Provisioning"},{"location":"storage/docs/spark/ebs/#eks-admin-tasks","text":"First, create your EBS volumes: aws ec2 --region <region> create-volume --availability-zone <availability zone> --size 50 { \"AvailabilityZone\": \"<availability zone>\", \"MultiAttachEnabled\": false, \"Tags\": [], \"Encrypted\": false, \"VolumeType\": \"gp2\", \"VolumeId\": \"<vol -id>\", \"State\": \"creating\", \"Iops\": 150, \"SnapshotId\": \"\", \"CreateTime\": \"2020-11-03T18:36:21.000Z\", \"Size\": 50 } Create Persistent Volume(PV) that has the EBS volume created above hardcoded: cat > ebs - static - pv . yaml << EOF apiVersion : v1 kind : PersistentVolume metadata : name : ebs - static - pv spec : capacity : storage : 5 Gi accessModes : - ReadWriteOnce storageClassName : gp2 awsElasticBlockStore : fsType : ext4 volumeID : < vol - id > EOF kubectl apply - f ebs - static - pv . yaml - n < namespace > Create Persistent Volume Claim(PVC) for the Persistent Volume created above: cat > ebs - static - pvc . yaml << EOF kind : PersistentVolumeClaim apiVersion : v1 metadata : name : ebs - static - pvc spec : accessModes : - ReadWriteOnce resources : requests : storage : 5 Gi volumeName : ebs - static - pv EOF kubectl apply - f ebs - static - pvc . yaml - n < namespace > PVC - ebs-static-pvc can be used by spark developer to mount to the spark pod NOTE : Pods running in EKS worker nodes can only attach to the EBS volume provisioned in the same AZ as the EKS worker node. Use node selectors to schedule pods on EKS worker nodes the specified AZ.","title":"EKS Admin Tasks"},{"location":"storage/docs/spark/ebs/#spark-developer-tasks","text":"Request cat >spark-python-in-s3-ebs-static-localdir.json << EOF { \"name\": \"spark-python-in-s3-ebs-static-localdir\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.instances=10 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 \" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.options.claimName\":\"ebs-static-pvc\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.readOnly\":\"false\", } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-ebs-static-localdir.json Observed Behavior: When the job gets started, the pre-provisioned EBS volume is mounted to driver pod. You can exec into the driver container to verify that the EBS volume is mounted. Also you can verify the mount from the driver pod's spec. kubectl get pod <driver pod name> -n <namespace> -o yaml --export","title":"Spark Developer Tasks"},{"location":"storage/docs/spark/ebs/#dynamic-provisioning","text":"","title":"Dynamic Provisioning"},{"location":"storage/docs/spark/ebs/#eks-admin-tasks_1","text":"Create EBS Storage Class cat > demo - gp2 - sc . yaml << EOF apiVersion : storage . k8s . io / v1 kind : StorageClass metadata : name : demo - gp2 - sc provisioner : kubernetes . io / aws - ebs parameters : type : gp2 reclaimPolicy : Retain allowVolumeExpansion : true mountOptions : - debug volumeBindingMode : Immediate EOF kubectl apply - f demo - gp2 - sc . yaml create Persistent Volume for the EBS storage class - demo-gp2-sc cat > ebs - demo - gp2 - claim . yaml << EOF apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ebs - demo - gp2 - claim labels : app : chicago spec : storageClassName : demo - gp2 - sc accessModes : - ReadWriteOnce resources : requests : storage : 100 Gi EOF kubectl apply - f ebs - demo - gp2 - claim . yaml - n < namespace >","title":"EKS Admin Tasks"},{"location":"storage/docs/spark/ebs/#spark-developer-tasks_1","text":"Request cat >spark-python-in-s3-ebs-dynamic-localdir.json << EOF { \"name\": \"spark-python-in-s3-ebs-dynamic-localdir\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.instances=10 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.options.claimName\":\"ebs-demo-gp2-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-sparkspill.mount.readOnly\":\"false\", } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-ebs-dynamic-localdir.json Observed Behavior: When the job gets started an EBS volume is provisioned dynamically by the EBS CSI driver and mounted to the driver pod. You can exec into the driver container to verify that the EBS volume is mounted. Also, you can verify the mount from driver pod spec. kubectl get pod <driver pod name> -n <namespace> -o yaml --export POINT TO NOTE : It is not possible to use this dynamic provisioning strategy for EBS to spark executors. It is not possible to mount a new EBS volume to every Spark executor. Instead use a distributed file system like Lustre, EFS, NFS to mount to executors.","title":"Spark Developer Tasks"},{"location":"storage/docs/spark/fsx-lustre/","text":"EMR Containers integration with FSx for Lustre \u00b6 Amazon EKS clusters provide the compute and ephemeral storage for Spark workloads. Ephemeral storage provided by EKS is allocated from the EKS worker node's disk storage and the lifecycle of the storage is bound by the lifecycle of the driver and executor pod. Need for durable storage: When multiple spark applications are executed as part of a data pipeline, there are scenarios where data from one spark application is passed to subsequent spark applications - in this case data can be persisted in S3. Alternatively, this data can also be persisted in FSx for Lustre . FSx for Lustre provides a fully managed, scalable, POSIX compliant native filesystem interface for the data in s3. With FSx, your torage is decoupled from your compute and has its own lifecycle. FSx for Lustre Volumes can be mounted on spark driver and executor pods through static and dynamic provisioning. Data used in the below example is from AWS Open data Registry FSx for Lustre POSIX permissions \u00b6 When a Lustre filesystem is mounted to driver and executor pods, and if the S3 objects does not have required metadata, the mounted volume defaults ownership of the file system to root . EMR on EKS executes the driver and executor pods with UID(999), GID (1000) and groups(1000 and 65534). In this scenario, the spark application has read only access to the mounted Lustre file system. Below are a few approaches that can be considered: Tag Metadata to S3 object \u00b6 Applications writing to S3 can tag the S3 objects with the metadata that FSx for Lustre requires. Walkthrough: Attaching POSIX permissions when uploading objects into an S3 bucket provides a guided tutorial. FSx for Lustre will convert this tagged metadata to corresponding POSIX permissions when mounting Lustre file system to the driver and executor pods. EMR on EKS spawns the driver and executor pods as non-root user( UID -999, GID - 1000, groups - 1000, 65534 ). To enable the spark application to write to the mounted file system, (UID - 999 ) can be made as the file-owner and supplemental group 65534 be made as the file-group . For S3 objects that already exists with no metadata tagging, there can be a process that recursively tags all the S3 objects with the required metadata. Below is an example: 1. Create FSx for Lustre file system to the S3 prefix. 2. Create Persistent Volume and Persistent Volume claim for the created FSx for Lustre file system 3. Run a pod as root user with FSx for Lustre mounted with the PVC created in Step 2. ``` apiVersion: v1 kind: Pod metadata: name: chmod-fsx-pod namespace: test-demo spec: containers: - name: ownership-change image: amazonlinux:2 command: [\"sh\", \"-c\", \"chown -hR +999:+65534 /data\"] volumeMounts: - name: persistent-storage mountPath: /data volumes: - name: persistent-storage persistentVolumeClaim: claimName: fsx-static-root-claim ``` Run a data repository task with import path and export path pointing to the same S3 prefix. This will export the POSIX permission from FSx for Lustre file system as metadata, that is tagged on S3 objects. Now that the S3 objects are tagged with metadata, the spark application with FSx for Lustre filesystem mounted will have write access. Static Provisioning \u00b6 Provision a FSx for Lustre cluster \u00b6 FSx for Luster can also be provisioned through aws cli How to decide what type of FSx for Lustre file system you need ? Create a Security Group to attach to FSx for Lustre file system as below Points to Note: Security group attached to the EKS worker nodes is given access on port number 988, 1021-1023 in inbound rules. Security group specified when creating the FSx for Lustre filesystem is given access on port number 988, 1021-1023 in inbound rules. Fsx for Lustre Provisioning through aws cli cat fsxLustreConfig.json << EOF { \"ClientRequestToken\": \"EMRContainers-fsxLustre-demo\", \"FileSystemType\": \"LUSTRE\", \"StorageCapacity\": 1200, \"StorageType\": \"SSD\", \"SubnetIds\": [ \"<subnet-id>\" ], \"SecurityGroupIds\": [ \"<securitygroup-id>\" ], \"LustreConfiguration\": { \"ImportPath\": \"s3://<s3 prefix>/\", \"ExportPath\": \"s3://<s3 prefix>/\", \"DeploymentType\": \"PERSISTENT_1\", \"AutoImportPolicy\": \"NEW_CHANGED\", \"PerUnitStorageThroughput\": 200 } } EOF Run the aws-cli command to create the FSx for Lustre filesystem as below. aws fsx create-file-system --cli-input-json file:///fsxLustreConfig.json Response is as below { \"FileSystem\": { \"VpcId\": \"<vpc id>\", \"Tags\": [], \"StorageType\": \"SSD\", \"SubnetIds\": [ \"<subnet-id>\" ], \"FileSystemType\": \"LUSTRE\", \"CreationTime\": 1603752401.183, \"ResourceARN\": \"<fsx resource arn>\", \"StorageCapacity\": 1200, \"LustreConfiguration\": { \"CopyTagsToBackups\": false, \"WeeklyMaintenanceStartTime\": \"7:11:30\", \"DataRepositoryConfiguration\": { \"ImportPath\": \"s3://<s3 prefix>\", \"AutoImportPolicy\": \"NEW_CHANGED\", \"ImportedFileChunkSize\": 1024, \"Lifecycle\": \"CREATING\", \"ExportPath\": \"s3://<s3 prefix>/\" }, \"DeploymentType\": \"PERSISTENT_1\", \"PerUnitStorageThroughput\": 200, \"MountName\": \"mvmxtbmv\" }, \"FileSystemId\": \"<filesystem id>\", \"DNSName\": \"<filesystem id>.fsx.<region>.amazonaws.com\", \"KmsKeyId\": \"arn:aws:kms:<region>:<account>:key/<key id>\", \"OwnerId\": \"<account>\", \"Lifecycle\": \"CREATING\" } } EKS admin tasks \u00b6 Attach IAM policy to EKS worker node IAM role to enable access to FSx for Lustre - Mount FSx for Lustre on EKS and Create a Security Group for FSx for Lustre Install the FSx CSI Driver in EKS Configure Storage Class for FSx for Lustre Configure Persistent Volume and Persistent Volume Claim for FSx for Lustre FSx for Lustre file system is created as described above - Provision a FSx for Lustre cluster Once provisioned, a persistent volume - as specified below is created with a direct (hard-coded) reference to the created lustre file system. A Persistent Volume claim for this persistent volume will always use the same file system. cat >fsxLustre-static-pv.yaml <<EOF apiVersion: v1 kind: PersistentVolume metadata: name: fsx-pv spec: capacity: storage: 1200Gi volumeMode: Filesystem accessModes: - ReadWriteMany mountOptions: - flock persistentVolumeReclaimPolicy: Recycle csi: driver: fsx.csi.aws.com volumeHandle: <filesystem id> volumeAttributes: dnsname: <filesystem id>.fsx.<region>.amazonaws.com mountname: mvmxtbmv EOF kubectl apply -f fsxLustre-static-pv.yaml Now, a Persistent Volume Claim (PVC) needs to be created that references PV created above. cat >fsxLustre-static-pvc.yaml <<EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: fsx-claim namespace: ns1 spec: accessModes: - ReadWriteMany storageClassName: \"\" resources: requests: storage: 1200Gi volumeName: fsx-pv EOF kubectl apply -f fsxLustre-static-pvc.yaml -n <namespace registered with EMR on EKS Virtual Cluster> Spark Developer Tasks \u00b6 Now spark applications can use fsx-claim in their spark application config to mount the FSx for Lustre filesystem to driver and executor container volumes. cat >spark-python-in-s3-fsx.json <<EOF { \"name\": \"spark-python-in-s3-fsx\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-repartition-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///Spark-Python-in-s3-fsx.json Expected Behavior: All spark jobs that are run with persistent volume claims as fsx-claim will mount to the statically created FSx for Lustre file system. Use case: A data pipeline consisting of 10 spark applications can all be mounted to the statically created FSx for Lustre file system and can write the intermediate output to a particular folder. The next spark job in the data pipeline that is dependent on this data can read from FSx for Lustre. Data that needs to be persisted beyond the scope of the data pipeline can be exported to S3 by creating data repository tasks Data that is used often by multiple spark applications can also be stored in FSx for Lustre for improved performance. Dynamic Provisioning \u00b6 A FSx for Lustre file system can be provisioned on-demand. A Storage-class resource is created and that provisions FSx for Lustre file system dynamically. A PVC is created and refers to the storage class resource that was created. Whenever a pod refers to the PVC, the storage class invokes the FSx for Lustre Container Storage Interface (CSI) to provision a Lustre file system on the fly dynamically. In this model, FSx for Lustre of type Scratch File Systems is provisioned. EKS Admin Tasks \u00b6 Attach IAM policy to EKS worker node IAM role to enable access to FSx for Lustre - Mount FSx for Lustre on EKS and Create a Security Group for FSx for Lustre Install the FSx CSI Driver in EKS Configure Storage Class for FSx for Lustre Configure Persistent Volume Claim( fsx-dynamic-claim ) for FSx for Lustre. Create PVC for dynamic provisioning with fsx-sc storage class. cat >fsx-dynamic-claim.yaml <<EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: fsx-dynamic-claim spec: accessModes: - ReadWriteMany storageClassName: fsx-sc resources: requests: storage: 3600Gi EOF kubectl apply -f fsx-dynamic-pvc.yaml -n <namespace registered with EMR on EKS Virtual Cluster> Spark Developer Tasks \u00b6 cat >spark-python-in-s3-fsx-dynamic.json << EOF { \"name\": \"spark-python-in-s3-fsx-dynamic\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-repartition-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.kubernetes.pyspark.pythonVersion=3 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 --conf spark.sql.shuffle.partitions=1000\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.local.dir\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.options.claimName\":\"fsx-dynamic-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///Spark-Python-in-s3-fsx-dynamic.json Expected Result: Statically provisioned FSx for Lustre is mounted to /var/data/ as before for the driver pod. For all the executors a SCRATCH 1 deployment type FSx for Lustre is provisioned on the fly dynamically by the Storage class that was created. There will be a latency before the first executor can start running - because the Lustre has to be created. Once it is created the same Lustre file system is mounted to all the executors. Also note - \"spark.local.dir\":\"/var/spark/spill/\" is used to force executor to use this folder mounted to Lustre for all spill and shuffle data. Once the spark job is completed, the Lustre file system is deleted or retained based on the PVC configuration. This dynamically created Lustre file system is mapped to a S3 path like the statically created filesystem. FSx-csi user guide","title":"FSx for Lustre"},{"location":"storage/docs/spark/fsx-lustre/#emr-containers-integration-with-fsx-for-lustre","text":"Amazon EKS clusters provide the compute and ephemeral storage for Spark workloads. Ephemeral storage provided by EKS is allocated from the EKS worker node's disk storage and the lifecycle of the storage is bound by the lifecycle of the driver and executor pod. Need for durable storage: When multiple spark applications are executed as part of a data pipeline, there are scenarios where data from one spark application is passed to subsequent spark applications - in this case data can be persisted in S3. Alternatively, this data can also be persisted in FSx for Lustre . FSx for Lustre provides a fully managed, scalable, POSIX compliant native filesystem interface for the data in s3. With FSx, your torage is decoupled from your compute and has its own lifecycle. FSx for Lustre Volumes can be mounted on spark driver and executor pods through static and dynamic provisioning. Data used in the below example is from AWS Open data Registry","title":"EMR Containers integration with FSx for Lustre"},{"location":"storage/docs/spark/fsx-lustre/#fsx-for-lustre-posix-permissions","text":"When a Lustre filesystem is mounted to driver and executor pods, and if the S3 objects does not have required metadata, the mounted volume defaults ownership of the file system to root . EMR on EKS executes the driver and executor pods with UID(999), GID (1000) and groups(1000 and 65534). In this scenario, the spark application has read only access to the mounted Lustre file system. Below are a few approaches that can be considered:","title":"FSx for Lustre POSIX permissions"},{"location":"storage/docs/spark/fsx-lustre/#tag-metadata-to-s3-object","text":"Applications writing to S3 can tag the S3 objects with the metadata that FSx for Lustre requires. Walkthrough: Attaching POSIX permissions when uploading objects into an S3 bucket provides a guided tutorial. FSx for Lustre will convert this tagged metadata to corresponding POSIX permissions when mounting Lustre file system to the driver and executor pods. EMR on EKS spawns the driver and executor pods as non-root user( UID -999, GID - 1000, groups - 1000, 65534 ). To enable the spark application to write to the mounted file system, (UID - 999 ) can be made as the file-owner and supplemental group 65534 be made as the file-group . For S3 objects that already exists with no metadata tagging, there can be a process that recursively tags all the S3 objects with the required metadata. Below is an example: 1. Create FSx for Lustre file system to the S3 prefix. 2. Create Persistent Volume and Persistent Volume claim for the created FSx for Lustre file system 3. Run a pod as root user with FSx for Lustre mounted with the PVC created in Step 2. ``` apiVersion: v1 kind: Pod metadata: name: chmod-fsx-pod namespace: test-demo spec: containers: - name: ownership-change image: amazonlinux:2 command: [\"sh\", \"-c\", \"chown -hR +999:+65534 /data\"] volumeMounts: - name: persistent-storage mountPath: /data volumes: - name: persistent-storage persistentVolumeClaim: claimName: fsx-static-root-claim ``` Run a data repository task with import path and export path pointing to the same S3 prefix. This will export the POSIX permission from FSx for Lustre file system as metadata, that is tagged on S3 objects. Now that the S3 objects are tagged with metadata, the spark application with FSx for Lustre filesystem mounted will have write access.","title":"Tag Metadata to S3 object"},{"location":"storage/docs/spark/fsx-lustre/#static-provisioning","text":"","title":"Static Provisioning"},{"location":"storage/docs/spark/fsx-lustre/#provision-a-fsx-for-lustre-cluster","text":"FSx for Luster can also be provisioned through aws cli How to decide what type of FSx for Lustre file system you need ? Create a Security Group to attach to FSx for Lustre file system as below Points to Note: Security group attached to the EKS worker nodes is given access on port number 988, 1021-1023 in inbound rules. Security group specified when creating the FSx for Lustre filesystem is given access on port number 988, 1021-1023 in inbound rules. Fsx for Lustre Provisioning through aws cli cat fsxLustreConfig.json << EOF { \"ClientRequestToken\": \"EMRContainers-fsxLustre-demo\", \"FileSystemType\": \"LUSTRE\", \"StorageCapacity\": 1200, \"StorageType\": \"SSD\", \"SubnetIds\": [ \"<subnet-id>\" ], \"SecurityGroupIds\": [ \"<securitygroup-id>\" ], \"LustreConfiguration\": { \"ImportPath\": \"s3://<s3 prefix>/\", \"ExportPath\": \"s3://<s3 prefix>/\", \"DeploymentType\": \"PERSISTENT_1\", \"AutoImportPolicy\": \"NEW_CHANGED\", \"PerUnitStorageThroughput\": 200 } } EOF Run the aws-cli command to create the FSx for Lustre filesystem as below. aws fsx create-file-system --cli-input-json file:///fsxLustreConfig.json Response is as below { \"FileSystem\": { \"VpcId\": \"<vpc id>\", \"Tags\": [], \"StorageType\": \"SSD\", \"SubnetIds\": [ \"<subnet-id>\" ], \"FileSystemType\": \"LUSTRE\", \"CreationTime\": 1603752401.183, \"ResourceARN\": \"<fsx resource arn>\", \"StorageCapacity\": 1200, \"LustreConfiguration\": { \"CopyTagsToBackups\": false, \"WeeklyMaintenanceStartTime\": \"7:11:30\", \"DataRepositoryConfiguration\": { \"ImportPath\": \"s3://<s3 prefix>\", \"AutoImportPolicy\": \"NEW_CHANGED\", \"ImportedFileChunkSize\": 1024, \"Lifecycle\": \"CREATING\", \"ExportPath\": \"s3://<s3 prefix>/\" }, \"DeploymentType\": \"PERSISTENT_1\", \"PerUnitStorageThroughput\": 200, \"MountName\": \"mvmxtbmv\" }, \"FileSystemId\": \"<filesystem id>\", \"DNSName\": \"<filesystem id>.fsx.<region>.amazonaws.com\", \"KmsKeyId\": \"arn:aws:kms:<region>:<account>:key/<key id>\", \"OwnerId\": \"<account>\", \"Lifecycle\": \"CREATING\" } }","title":"Provision a FSx for Lustre cluster"},{"location":"storage/docs/spark/fsx-lustre/#eks-admin-tasks","text":"Attach IAM policy to EKS worker node IAM role to enable access to FSx for Lustre - Mount FSx for Lustre on EKS and Create a Security Group for FSx for Lustre Install the FSx CSI Driver in EKS Configure Storage Class for FSx for Lustre Configure Persistent Volume and Persistent Volume Claim for FSx for Lustre FSx for Lustre file system is created as described above - Provision a FSx for Lustre cluster Once provisioned, a persistent volume - as specified below is created with a direct (hard-coded) reference to the created lustre file system. A Persistent Volume claim for this persistent volume will always use the same file system. cat >fsxLustre-static-pv.yaml <<EOF apiVersion: v1 kind: PersistentVolume metadata: name: fsx-pv spec: capacity: storage: 1200Gi volumeMode: Filesystem accessModes: - ReadWriteMany mountOptions: - flock persistentVolumeReclaimPolicy: Recycle csi: driver: fsx.csi.aws.com volumeHandle: <filesystem id> volumeAttributes: dnsname: <filesystem id>.fsx.<region>.amazonaws.com mountname: mvmxtbmv EOF kubectl apply -f fsxLustre-static-pv.yaml Now, a Persistent Volume Claim (PVC) needs to be created that references PV created above. cat >fsxLustre-static-pvc.yaml <<EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: fsx-claim namespace: ns1 spec: accessModes: - ReadWriteMany storageClassName: \"\" resources: requests: storage: 1200Gi volumeName: fsx-pv EOF kubectl apply -f fsxLustre-static-pvc.yaml -n <namespace registered with EMR on EKS Virtual Cluster>","title":"EKS admin tasks"},{"location":"storage/docs/spark/fsx-lustre/#spark-developer-tasks","text":"Now spark applications can use fsx-claim in their spark application config to mount the FSx for Lustre filesystem to driver and executor container volumes. cat >spark-python-in-s3-fsx.json <<EOF { \"name\": \"spark-python-in-s3-fsx\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-repartition-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///Spark-Python-in-s3-fsx.json Expected Behavior: All spark jobs that are run with persistent volume claims as fsx-claim will mount to the statically created FSx for Lustre file system. Use case: A data pipeline consisting of 10 spark applications can all be mounted to the statically created FSx for Lustre file system and can write the intermediate output to a particular folder. The next spark job in the data pipeline that is dependent on this data can read from FSx for Lustre. Data that needs to be persisted beyond the scope of the data pipeline can be exported to S3 by creating data repository tasks Data that is used often by multiple spark applications can also be stored in FSx for Lustre for improved performance.","title":"Spark Developer Tasks"},{"location":"storage/docs/spark/fsx-lustre/#dynamic-provisioning","text":"A FSx for Lustre file system can be provisioned on-demand. A Storage-class resource is created and that provisions FSx for Lustre file system dynamically. A PVC is created and refers to the storage class resource that was created. Whenever a pod refers to the PVC, the storage class invokes the FSx for Lustre Container Storage Interface (CSI) to provision a Lustre file system on the fly dynamically. In this model, FSx for Lustre of type Scratch File Systems is provisioned.","title":"Dynamic Provisioning"},{"location":"storage/docs/spark/fsx-lustre/#eks-admin-tasks_1","text":"Attach IAM policy to EKS worker node IAM role to enable access to FSx for Lustre - Mount FSx for Lustre on EKS and Create a Security Group for FSx for Lustre Install the FSx CSI Driver in EKS Configure Storage Class for FSx for Lustre Configure Persistent Volume Claim( fsx-dynamic-claim ) for FSx for Lustre. Create PVC for dynamic provisioning with fsx-sc storage class. cat >fsx-dynamic-claim.yaml <<EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: fsx-dynamic-claim spec: accessModes: - ReadWriteMany storageClassName: fsx-sc resources: requests: storage: 3600Gi EOF kubectl apply -f fsx-dynamic-pvc.yaml -n <namespace registered with EMR on EKS Virtual Cluster>","title":"EKS Admin Tasks"},{"location":"storage/docs/spark/fsx-lustre/#spark-developer-tasks_1","text":"cat >spark-python-in-s3-fsx-dynamic.json << EOF { \"name\": \"spark-python-in-s3-fsx-dynamic\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/trip-count-repartition-fsx.py\", \"sparkSubmitParameters\": \"--conf spark.driver.cores=5 --conf spark.kubernetes.pyspark.pythonVersion=3 --conf spark.executor.memory=20G --conf spark.driver.memory=15G --conf spark.executor.cores=6 --conf spark.sql.shuffle.partitions=1000\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.local.dir\":\"/var/spark/spill/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.options.claimName\":\"fsx-dynamic-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.mount.path\":\"/var/spark/spill/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-spill.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } EOF aws emr-containers start-job-run --cli-input-json file:///Spark-Python-in-s3-fsx-dynamic.json Expected Result: Statically provisioned FSx for Lustre is mounted to /var/data/ as before for the driver pod. For all the executors a SCRATCH 1 deployment type FSx for Lustre is provisioned on the fly dynamically by the Storage class that was created. There will be a latency before the first executor can start running - because the Lustre has to be created. Once it is created the same Lustre file system is mounted to all the executors. Also note - \"spark.local.dir\":\"/var/spark/spill/\" is used to force executor to use this folder mounted to Lustre for all spill and shuffle data. Once the spark job is completed, the Lustre file system is deleted or retained based on the PVC configuration. This dynamically created Lustre file system is mapped to a S3 path like the statically created filesystem. FSx-csi user guide","title":"Spark Developer Tasks"},{"location":"submit-applications/docs/spark/","text":"","title":"Index"},{"location":"submit-applications/docs/spark/java-and-scala/","text":"","title":"Java and scala"},{"location":"submit-applications/docs/spark/pyspark/","text":"Pyspark Job submission \u00b6 Python interpreter is bundled in the EMR containers spark image that is used to run the spark job.Python code and dependencies can be provided with the below options. Python code self contained in a single .py file \u00b6 To start with, in the most simplest scenario - the example below shows how to submit a pi.py file that is self contained and doesn't need any other dependencies. Python file from S3 \u00b6 Request pi.py used in the below request payload is from spark examples cat > spark - python - in - s3 . json << EOF { \"name\" : \"spark-python-in-image\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3.json Python file from mounted volume \u00b6 In the below example - pi.py is placed in a mounted volume. FSx for Lustre filesystem is mounted as a Persistent Volume on the driver pod under /var/data/ and will be referenced by local:// file prefix. For more information on how to mount FSx for lustre - EMR-Containers-integration-with-FSx-for-Lustre This approach can be used to provide spark application code and dependencies for execution. Persistent Volume mounted to the driver and executor pods lets you access the application code and dependencies with local:// prefix. cat > spark - python - in - FSx . json << EOF { \"name\" : \"spark-python-in-FSx\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"local:///var/data/FSxLustre-pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-Fsx.json Python code with dependencies \u00b6 List of .py files \u00b6 This is not a scalable approach as the number of dependent files can grow to a large number, and also need to manually specify all of the transitive dependencies. cat > py - files - pi . py << EOF from __future__ import print_function import sys from random import random from operator import add from pyspark.sql import SparkSession from pyspark import SparkContext import dependentFunc if __name__ == \"__main__\" : \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext partitions = int ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else 2 n = 100000 * partitions def f ( _ ): x = random () * 2 - 1 y = random () * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark . sparkContext . parallelize ( range ( 1 , n + 1 ), partitions ) . map ( f ) . reduce ( add ) dependentFunc . message () print ( \"Pi is roughly %f \" % ( 4.0 * count / n )) spark . stop () EOF cat > dependentFunc . py << EOF def message (): print ( \"Printing from inside the dependent python file\" ) EOF Upload dependentFunc.py and py-files-pi.py to s3 Request: cat > spark - python - in - s3 - dependency - files << EOF { \"name\" : \"spark-python-in-s3-dependency-files\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/dependentFunc.py --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-files.json Bundled as a zip file \u00b6 In this approach all the dependent python files are bundled as a zip file. Each folder should have __init__.py file as documented in zip python dependencies . Zip should be done at the top folder level and using the -r option. zip -r pyspark-packaged-dependency-src.zip . adding: dependent/ (stored 0%) adding: dependent/__init__.py (stored 0%) adding: dependent/dependentFunc.py (deflated 7%) dependentFunc.py from earlier example has been bundled as pyspark-packaged-dependency-src.zip . Upload this file to a S3 location cat > py - files - zip - pi . py << EOF from __future__ import print_function import sys from random import random from operator import add from pyspark.sql import SparkSession from pyspark import SparkContext ** from dependent import dependentFunc ** if __name__ == \"__main__\" : \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext partitions = int ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else 2 n = 100000 * partitions def f ( _ ): x = random () * 2 - 1 y = random () * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark . sparkContext . parallelize ( range ( 1 , n + 1 ), partitions ) . map ( f ) . reduce ( add ) dependentFunc . message () print ( \"Pi is roughly %f \" % ( 4.0 * count / n )) spark . stop () EOF Request: cat > spark - python - in - s3 - dependency - zip . json << EOF { \"name\" : \"spark-python-in-s3-dependency-zip\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark-packaged-dependency-src.zip --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-zip.json Bundled as a .egg file \u00b6 Create a folder structure as in the below screenshot with the code from the previous example - py-files-zip-pi.py, dependentFunc.py Steps to create .egg file cd /pyspark-packaged-example pip install setuptools python setup.py bdist_egg Upload dist/pyspark_packaged_example-0.0.3-py3.8.egg to a S3 location Request: cat > spark - python - in - s3 - dependency - egg . json << EOF { \"name\" : \"spark-python-in-s3-dependency-egg\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark_packaged_example-0.0.3-py3.8.egg --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-egg.json Bundled as a .whl file \u00b6 Create a folder structure as in the below screenshot with the code from the previous example - py-files-zip-pi.py, dependentFunc.py Steps to create .whl file cd /pyspark-packaged-example `pip install wheel` python setup.py bdist_wheel Upload dist/pyspark_packaged_example-0.0.3-py3-none-any.whl to a s3 location Request: cat > spark - python - in - s3 - dependency - wheel . json << EOF { \"name\" : \"spark-python-in-s3-dependency-wheel\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark_packaged_example-0.0.3-py3-none-any.whl --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-wheel.json Bundled as a .pex file \u00b6 pex is a library for generating .pex (Python EXecutable) files which are executable Python environments.PEX files can be created as below docker run -it -v $(pwd):/workdir python:3.7.9-buster /bin/bash #python 3.7.9 is installed in EMR 6.1.0 pip3 install pex pex --python=python3 --inherit-path=prefer -v numpy -o numpy_dep.pex To read more about PEX: PEX PEX documentation Tips on PEX pex packaging for pyspark Upload numpy_dep.pex to a s3 location that is mapped to a FSx for Lustre cluster. numpy_dep.pex can be placed on any Kubernetes persistent volume and mounted to the driver pod and executor pod. Alternatively, S3 path for numpy_dep.pex can also be passed using --py-files Request: kmeans.py used in the below request is from spark examples cat > spark - python - in - s3 - pex - fsx . json << EOF { \"name\" : \"spark-python-in-s3-pex-fsx\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/kmeans.py\" , \"entryPointArguments\" : [ \"s3://<s3 prefix>/kmeans_data.txt\" , \"2\" , \"3\" ], \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.kubernetes.pyspark.pythonVersion\" : \"3\" , \"spark.kubernetes.driverEnv.PEX_ROOT\" : \"./tmp\" , \"spark.executorEnv.PEX_ROOT\" : \"./tmp\" , \"spark.kubernetes.driverEnv.PEX_INHERIT_PATH\" : \"prefer\" , \"spark.executorEnv.PEX_INHERIT_PATH\" : \"prefer\" , \"spark.kubernetes.driverEnv.PEX_VERBOSE\" : \"10\" , \"spark.kubernetes.driverEnv.PEX_PYTHON\" : \"python3\" , \"spark.executorEnv.PEX_PYTHON\" : \"python3\" , \"spark.pyspark.driver.python\" : \"/var/data/numpy_dep.pex\" , \"spark.pyspark.python\" : \"/var/data/numpy_dep.pex\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } aws emr - containers start - job - run --cli-input-json file:////Spark-Python-in-s3-pex-fsx.json Point to Note: PEX files don\u2019t have the python interpreter bundled with it. Using the PEX env variables, we pass in the python interpreter installed in the spark driver and executor docker image. pex vs conda-pack A pex file contain only dependent Python packages but not a Python interpreter in it while a conda-pack environment has a Python interpreter as well, so with the same Python packages a conda-pack environment is much larger than a pex file. A conda-pack environment is a tar.gz file and need to be decompressed before being used while a pex file can be used directly. If a Python interpreter exists, pex is a better option than conda-pack. However, conda-pack is the ONLY CHOICE if you need a specific version of Python interpreter which does not exist and you do not have permission to install one (e.g., when you need to use a specific version of Python interpreter with an enterprise PySpark cluster). If the pex file or conda-pack environment needs to be distributed to machines on demand, there are some overhead before running your application. With the same Python packages, a conda-pack environment has large overhead/latency than the pex file as the conda-pack environment is usually much larger and need to be decompressed before being used. For more information - Tips on PEX Bundled as a tar.gz file with conda-pack \u00b6 conda-pack for spark Install conda through Miniconda Open a new terminal and execute the below commands conda create -y -n example python=3.5 numpy conda activate example pip install conda-pack conda pack -f -o numpy_environment.tar.gz Upload numpy_environment.tar.gz to a s3 location that is mapped to a FSx for Lustre cluster. numpy_environment.tar.gz can be placed on any Kubernetes persistent volume and mounted to the driver pod and executor pod.Alternatively, S3 path for numpy_environment.tar.gz can also be passed using --py-files Request: { \"name\": \"spark-python-in-s3-conda-fsx\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/kmeans.py\", \"entryPointArguments\": [ \"s3://<s3 prefix>/kmeans_data.txt\", \"2\", \"3\" ], \"sparkSubmitParameters\": \"--verbose --archives /var/data/numpy_environment.tar.gz#environment --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.instances\": \"3\", \"spark.dynamicAllocation.enabled\":\"false\", \"spark.files\":\"/var/data/numpy_environment.tar.gz#environment\", \"spark.kubernetes.pyspark.pythonVersion\":\"3\", \"spark.pyspark.driver.python\":\"./environment/bin/python\", \"spark.pyspark.python\":\"./environment/bin/python\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } The above request doesn't work with spark on kubernetes Bundled as virtual env \u00b6 This will not work with spark on kubernetes .This feature only works with YARN - cluster mode In this implementation for YARN - the dependencies will be installed from the repository for every driver and executor. This might not be a more scalable model as per SPARK-25433 . Recommended solution is to pass in the dependencies as PEX file. Import of Dynamic Modules (.pyd, .so) \u00b6 Import of dynamic modules(.pyd, .so) is disallowed when bundled as a zip Steps to create a .so file example.c /* File : example.c */ #include \"example.h\" unsigned int add ( unsigned int a , unsigned int b ) { printf ( \" \\n Inside add function in C library \\n \" ); return ( a + b ); } example.h /* File : example.h */ #include<stdio.h> extern unsigned int add(unsigned int a, unsigned int b); gcc -fPIC -Wall -g -c example.c gcc -shared -fPIC -o libexample.so example.o Upload libexample.so to a S3 location. pyspark code to be executed - py_c_call.py import sys import os from ctypes import CDLL from pyspark.sql import SparkSession if __name__ == \"__main__\" : spark = SparkSession \\ . builder \\ . appName ( \"py-c-so-example\" ) \\ . getOrCreate () basedir = os . path . abspath ( os . path . dirname ( __file__ )) libpath = os . path . join ( basedir , 'libexample.so' ) sum_list = CDLL ( libpath ) data = [( 1 , 2 ),( 2 , 3 ),( 5 , 6 )] columns = [ \"a\" , \"b\" ] df = spark . sparkContext . parallelize ( data ) . toDF ( columns ) df . withColumn ( 'total' , sum_list . add ( df . a , df . b )) . collect () spark . stop () Request: cat > spark - python - in - s3 - Clib . json << EOF { \"name\" : \"spark-python-in-s3-Clib\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py_c_call.py\" , \"sparkSubmitParameters\" : \"--files s3://<s3 prefix>/libexample.so --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-Clib.json Configuration of interest: --files s3://<s3 prefix>/libexample.so distributes the libexample.so to the working directory of all executors. Dynamic modules(.pyd, .so) can also be imported by bundling within .egg ( SPARK-6764 ), .whl and .pex files.","title":"Pyspark"},{"location":"submit-applications/docs/spark/pyspark/#pyspark-job-submission","text":"Python interpreter is bundled in the EMR containers spark image that is used to run the spark job.Python code and dependencies can be provided with the below options.","title":"Pyspark Job submission"},{"location":"submit-applications/docs/spark/pyspark/#python-code-self-contained-in-a-single-py-file","text":"To start with, in the most simplest scenario - the example below shows how to submit a pi.py file that is self contained and doesn't need any other dependencies.","title":"Python code self contained in a single .py file"},{"location":"submit-applications/docs/spark/pyspark/#python-file-from-s3","text":"Request pi.py used in the below request payload is from spark examples cat > spark - python - in - s3 . json << EOF { \"name\" : \"spark-python-in-image\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-s3.json","title":"Python file from S3"},{"location":"submit-applications/docs/spark/pyspark/#python-file-from-mounted-volume","text":"In the below example - pi.py is placed in a mounted volume. FSx for Lustre filesystem is mounted as a Persistent Volume on the driver pod under /var/data/ and will be referenced by local:// file prefix. For more information on how to mount FSx for lustre - EMR-Containers-integration-with-FSx-for-Lustre This approach can be used to provide spark application code and dependencies for execution. Persistent Volume mounted to the driver and executor pods lets you access the application code and dependencies with local:// prefix. cat > spark - python - in - FSx . json << EOF { \"name\" : \"spark-python-in-FSx\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"local:///var/data/FSxLustre-pi.py\" , \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///Spark-Python-in-Fsx.json","title":"Python file from mounted volume"},{"location":"submit-applications/docs/spark/pyspark/#python-code-with-dependencies","text":"","title":"Python code with dependencies"},{"location":"submit-applications/docs/spark/pyspark/#list-of-py-files","text":"This is not a scalable approach as the number of dependent files can grow to a large number, and also need to manually specify all of the transitive dependencies. cat > py - files - pi . py << EOF from __future__ import print_function import sys from random import random from operator import add from pyspark.sql import SparkSession from pyspark import SparkContext import dependentFunc if __name__ == \"__main__\" : \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext partitions = int ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else 2 n = 100000 * partitions def f ( _ ): x = random () * 2 - 1 y = random () * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark . sparkContext . parallelize ( range ( 1 , n + 1 ), partitions ) . map ( f ) . reduce ( add ) dependentFunc . message () print ( \"Pi is roughly %f \" % ( 4.0 * count / n )) spark . stop () EOF cat > dependentFunc . py << EOF def message (): print ( \"Printing from inside the dependent python file\" ) EOF Upload dependentFunc.py and py-files-pi.py to s3 Request: cat > spark - python - in - s3 - dependency - files << EOF { \"name\" : \"spark-python-in-s3-dependency-files\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/dependentFunc.py --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-files.json","title":"List of .py files"},{"location":"submit-applications/docs/spark/pyspark/#bundled-as-a-zip-file","text":"In this approach all the dependent python files are bundled as a zip file. Each folder should have __init__.py file as documented in zip python dependencies . Zip should be done at the top folder level and using the -r option. zip -r pyspark-packaged-dependency-src.zip . adding: dependent/ (stored 0%) adding: dependent/__init__.py (stored 0%) adding: dependent/dependentFunc.py (deflated 7%) dependentFunc.py from earlier example has been bundled as pyspark-packaged-dependency-src.zip . Upload this file to a S3 location cat > py - files - zip - pi . py << EOF from __future__ import print_function import sys from random import random from operator import add from pyspark.sql import SparkSession from pyspark import SparkContext ** from dependent import dependentFunc ** if __name__ == \"__main__\" : \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext partitions = int ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else 2 n = 100000 * partitions def f ( _ ): x = random () * 2 - 1 y = random () * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark . sparkContext . parallelize ( range ( 1 , n + 1 ), partitions ) . map ( f ) . reduce ( add ) dependentFunc . message () print ( \"Pi is roughly %f \" % ( 4.0 * count / n )) spark . stop () EOF Request: cat > spark - python - in - s3 - dependency - zip . json << EOF { \"name\" : \"spark-python-in-s3-dependency-zip\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark-packaged-dependency-src.zip --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-zip.json","title":"Bundled as a zip file"},{"location":"submit-applications/docs/spark/pyspark/#bundled-as-a-egg-file","text":"Create a folder structure as in the below screenshot with the code from the previous example - py-files-zip-pi.py, dependentFunc.py Steps to create .egg file cd /pyspark-packaged-example pip install setuptools python setup.py bdist_egg Upload dist/pyspark_packaged_example-0.0.3-py3.8.egg to a S3 location Request: cat > spark - python - in - s3 - dependency - egg . json << EOF { \"name\" : \"spark-python-in-s3-dependency-egg\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark_packaged_example-0.0.3-py3.8.egg --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-egg.json","title":"Bundled as a .egg file"},{"location":"submit-applications/docs/spark/pyspark/#bundled-as-a-whl-file","text":"Create a folder structure as in the below screenshot with the code from the previous example - py-files-zip-pi.py, dependentFunc.py Steps to create .whl file cd /pyspark-packaged-example `pip install wheel` python setup.py bdist_wheel Upload dist/pyspark_packaged_example-0.0.3-py3-none-any.whl to a s3 location Request: cat > spark - python - in - s3 - dependency - wheel . json << EOF { \"name\" : \"spark-python-in-s3-dependency-wheel\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py-files-zip-pi.py\" , \"sparkSubmitParameters\" : \"--py-files s3://<s3 prefix>/pyspark_packaged_example-0.0.3-py3-none-any.whl --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-dependency-wheel.json","title":"Bundled as a .whl file"},{"location":"submit-applications/docs/spark/pyspark/#bundled-as-a-pex-file","text":"pex is a library for generating .pex (Python EXecutable) files which are executable Python environments.PEX files can be created as below docker run -it -v $(pwd):/workdir python:3.7.9-buster /bin/bash #python 3.7.9 is installed in EMR 6.1.0 pip3 install pex pex --python=python3 --inherit-path=prefer -v numpy -o numpy_dep.pex To read more about PEX: PEX PEX documentation Tips on PEX pex packaging for pyspark Upload numpy_dep.pex to a s3 location that is mapped to a FSx for Lustre cluster. numpy_dep.pex can be placed on any Kubernetes persistent volume and mounted to the driver pod and executor pod. Alternatively, S3 path for numpy_dep.pex can also be passed using --py-files Request: kmeans.py used in the below request is from spark examples cat > spark - python - in - s3 - pex - fsx . json << EOF { \"name\" : \"spark-python-in-s3-pex-fsx\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/kmeans.py\" , \"entryPointArguments\" : [ \"s3://<s3 prefix>/kmeans_data.txt\" , \"2\" , \"3\" ], \"sparkSubmitParameters\" : \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.kubernetes.pyspark.pythonVersion\" : \"3\" , \"spark.kubernetes.driverEnv.PEX_ROOT\" : \"./tmp\" , \"spark.executorEnv.PEX_ROOT\" : \"./tmp\" , \"spark.kubernetes.driverEnv.PEX_INHERIT_PATH\" : \"prefer\" , \"spark.executorEnv.PEX_INHERIT_PATH\" : \"prefer\" , \"spark.kubernetes.driverEnv.PEX_VERBOSE\" : \"10\" , \"spark.kubernetes.driverEnv.PEX_PYTHON\" : \"python3\" , \"spark.executorEnv.PEX_PYTHON\" : \"python3\" , \"spark.pyspark.driver.python\" : \"/var/data/numpy_dep.pex\" , \"spark.pyspark.python\" : \"/var/data/numpy_dep.pex\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\" : \"fsx-claim\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\" : \"/var/data/\" , \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } aws emr - containers start - job - run --cli-input-json file:////Spark-Python-in-s3-pex-fsx.json Point to Note: PEX files don\u2019t have the python interpreter bundled with it. Using the PEX env variables, we pass in the python interpreter installed in the spark driver and executor docker image. pex vs conda-pack A pex file contain only dependent Python packages but not a Python interpreter in it while a conda-pack environment has a Python interpreter as well, so with the same Python packages a conda-pack environment is much larger than a pex file. A conda-pack environment is a tar.gz file and need to be decompressed before being used while a pex file can be used directly. If a Python interpreter exists, pex is a better option than conda-pack. However, conda-pack is the ONLY CHOICE if you need a specific version of Python interpreter which does not exist and you do not have permission to install one (e.g., when you need to use a specific version of Python interpreter with an enterprise PySpark cluster). If the pex file or conda-pack environment needs to be distributed to machines on demand, there are some overhead before running your application. With the same Python packages, a conda-pack environment has large overhead/latency than the pex file as the conda-pack environment is usually much larger and need to be decompressed before being used. For more information - Tips on PEX","title":"Bundled as a .pex file"},{"location":"submit-applications/docs/spark/pyspark/#bundled-as-a-targz-file-with-conda-pack","text":"conda-pack for spark Install conda through Miniconda Open a new terminal and execute the below commands conda create -y -n example python=3.5 numpy conda activate example pip install conda-pack conda pack -f -o numpy_environment.tar.gz Upload numpy_environment.tar.gz to a s3 location that is mapped to a FSx for Lustre cluster. numpy_environment.tar.gz can be placed on any Kubernetes persistent volume and mounted to the driver pod and executor pod.Alternatively, S3 path for numpy_environment.tar.gz can also be passed using --py-files Request: { \"name\": \"spark-python-in-s3-conda-fsx\", \"virtualClusterId\": \"<virtual-cluster-id>\", \"executionRoleArn\": \"<execution-role-arn>\", \"releaseLabel\": \"emr-6.2.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://<s3 prefix>/kmeans.py\", \"entryPointArguments\": [ \"s3://<s3 prefix>/kmeans_data.txt\", \"2\", \"3\" ], \"sparkSubmitParameters\": \"--verbose --archives /var/data/numpy_environment.tar.gz#environment --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [ { \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.instances\": \"3\", \"spark.dynamicAllocation.enabled\":\"false\", \"spark.files\":\"/var/data/numpy_environment.tar.gz#environment\", \"spark.kubernetes.pyspark.pythonVersion\":\"3\", \"spark.pyspark.driver.python\":\"./environment/bin/python\", \"spark.pyspark.python\":\"./environment/bin/python\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName\":\"fsx-claim\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path\":\"/var/data/\", \"spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly\":\"false\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"/emr-containers/jobs\", \"logStreamNamePrefix\": \"demo\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://joblogs\" } } } } The above request doesn't work with spark on kubernetes","title":"Bundled as a tar.gz file with conda-pack"},{"location":"submit-applications/docs/spark/pyspark/#bundled-as-virtual-env","text":"This will not work with spark on kubernetes .This feature only works with YARN - cluster mode In this implementation for YARN - the dependencies will be installed from the repository for every driver and executor. This might not be a more scalable model as per SPARK-25433 . Recommended solution is to pass in the dependencies as PEX file.","title":"Bundled as virtual env"},{"location":"submit-applications/docs/spark/pyspark/#import-of-dynamic-modules-pyd-so","text":"Import of dynamic modules(.pyd, .so) is disallowed when bundled as a zip Steps to create a .so file example.c /* File : example.c */ #include \"example.h\" unsigned int add ( unsigned int a , unsigned int b ) { printf ( \" \\n Inside add function in C library \\n \" ); return ( a + b ); } example.h /* File : example.h */ #include<stdio.h> extern unsigned int add(unsigned int a, unsigned int b); gcc -fPIC -Wall -g -c example.c gcc -shared -fPIC -o libexample.so example.o Upload libexample.so to a S3 location. pyspark code to be executed - py_c_call.py import sys import os from ctypes import CDLL from pyspark.sql import SparkSession if __name__ == \"__main__\" : spark = SparkSession \\ . builder \\ . appName ( \"py-c-so-example\" ) \\ . getOrCreate () basedir = os . path . abspath ( os . path . dirname ( __file__ )) libpath = os . path . join ( basedir , 'libexample.so' ) sum_list = CDLL ( libpath ) data = [( 1 , 2 ),( 2 , 3 ),( 5 , 6 )] columns = [ \"a\" , \"b\" ] df = spark . sparkContext . parallelize ( data ) . toDF ( columns ) df . withColumn ( 'total' , sum_list . add ( df . a , df . b )) . collect () spark . stop () Request: cat > spark - python - in - s3 - Clib . json << EOF { \"name\" : \"spark-python-in-s3-Clib\" , \"virtualClusterId\" : \"<virtual-cluster-id>\" , \"executionRoleArn\" : \"<execution-role-arn>\" , \"releaseLabel\" : \"emr-6.2.0-latest\" , \"jobDriver\" : { \"sparkSubmitJobDriver\" : { \"entryPoint\" : \"s3://<s3 prefix>/py_c_call.py\" , \"sparkSubmitParameters\" : \"--files s3://<s3 prefix>/libexample.so --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2\" } } , \"configurationOverrides\" : { \"applicationConfiguration\" : [ { \"classification\" : \"spark-defaults\" , \"properties\" : { \"spark.dynamicAllocation.enabled\" : \"false\" } } ], \"monitoringConfiguration\" : { \"cloudWatchMonitoringConfiguration\" : { \"logGroupName\" : \"/emr-containers/jobs\" , \"logStreamNamePrefix\" : \"demo\" } , \"s3MonitoringConfiguration\" : { \"logUri\" : \"s3://joblogs\" } } } } EOF aws emr - containers start - job - run --cli-input-json file:///spark-python-in-s3-Clib.json Configuration of interest: --files s3://<s3 prefix>/libexample.so distributes the libexample.so to the working directory of all executors. Dynamic modules(.pyd, .so) can also be imported by bundling within .egg ( SPARK-6764 ), .whl and .pex files.","title":"Import of Dynamic Modules (.pyd, .so)"},{"location":"submit-applications/docs/spark/sparkr/","text":"","title":"Sparkr"},{"location":"submit-applications/docs/spark/sparksql/","text":"","title":"Sparksql"}]}